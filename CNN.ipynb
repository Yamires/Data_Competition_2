{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEsbLdFkDI4D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "gQ69TXXpDMHG"
      },
      "outputs": [],
      "source": [
        "# Chemins vers les fichiers de données d'entraînement et de test\n",
        "DATA_PATH = \"ift-3395-6390-kaggle-2-competition-fall-2025/train_data.pkl\"\n",
        "TEST_PATH = \"ift-3395-6390-kaggle-2-competition-fall-2025/test_data.pkl\"\n",
        "\n",
        "# Chemin vers le fichier où le meilleur modèle sera sauvegardé\n",
        "MODEL_PATH = \"best_model_1.pth\"\n",
        "\n",
        "# Hyperparamètres globaux\n",
        "BATCH_SIZE = 64      # Taille des mini-lots\n",
        "NUM_CLASSES = 5      # Nombre de classes (0 à 4)\n",
        "NUM_EPOCHS = 20      # Nombre d'époques d'entraînement\n",
        "VAL_SIZE = 0.15      # Proportion des données utilisées pour la validation\n",
        "SEED = 42            # SEED aléatoire pour la reproductibilité\n",
        "\n",
        "# Hyperparamètres d'optimisation\n",
        "LR = 5e-4            # Learning rate\n",
        "WEIGHT_DECAY = 3e-4  # Terme de régularisation L2 \n",
        "\n",
        "\n",
        "DEVICE = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available() else\n",
        "    (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        ")\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    # Fixe les SEEDS aléatoires pour assurer la reproductibilité\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "# Initialisation des SEEDS aléatoires\n",
        "set_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay99l9vnDOG4",
        "outputId": "08f56809-b06f-45f2-bb47-ff408a1f7112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Images: (1080, 28, 28, 3)\n",
            "Labels: (1080,)\n"
          ]
        }
      ],
      "source": [
        "def load_data_pkl(path):\n",
        "    # Chargement des données depuis un fichier pickle\n",
        "    with open(path, \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    # Extraction des images\n",
        "    images = data[\"images\"].astype(np.uint8)\n",
        "\n",
        "    # Extraction des labels et conversion en tableau \n",
        "    labels = np.asarray(data[\"labels\"], dtype=np.int64).reshape(-1)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "# Chargement des données d'entraînement\n",
        "images, labels = load_data_pkl(DATA_PATH)\n",
        "\n",
        "# Vérification de la dimension des images (28x28 RGB)\n",
        "assert images.shape[1:] == (28, 28, 3), \"Les images doivent être en 28x28x3.\"\n",
        "\n",
        "# Affichage des dimensions pour validation\n",
        "print(\"\\nImages:\", images.shape)\n",
        "print(\"Labels:\", labels.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "WXoSAh_CDYGD"
      },
      "outputs": [],
      "source": [
        "class RetinaDataset(Dataset):\n",
        "    def __init__(self, images, labels=None, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels  \n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        # Retourne le nombre total d'images dans le dataset\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Conversion de l'image NumPy en image PIL\n",
        "        img = Image.fromarray(self.images[idx])\n",
        "\n",
        "        # Application des transformations \n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Retourne l'image et son label \n",
        "        if self.labels is not None:\n",
        "            return img, int(self.labels[idx])\n",
        "        else:\n",
        "            # Cas de l'ensemble de test (sans labels)\n",
        "            return img\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "DjbUQkvtDQ7B"
      },
      "outputs": [],
      "source": [
        "# Définition des transformations d'augmentation de données\n",
        "augment = transforms.Compose([\n",
        "    # Ajustement aléatoire du contraste\n",
        "    transforms.RandomAutocontrast(p=0.3),\n",
        "\n",
        "    # Application d'un flou gaussien léger\n",
        "    transforms.GaussianBlur(3, 0.3),\n",
        "\n",
        "    # Rotation aléatoire \n",
        "    transforms.RandomRotation(degrees=3),\n",
        "\n",
        "    # Flip horizontal aléatoire\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "    # Flip vertical aléatoire\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "\n",
        "    # Cisaillement\n",
        "    transforms.RandomAffine(degrees=0, shear=5),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "qd1-eiUvDSYa"
      },
      "outputs": [],
      "source": [
        "def preprocess_retina(img_tensor):\n",
        "    # Séparation des canaux de couleur (R, G, B)\n",
        "    R = img_tensor[0]\n",
        "    G = img_tensor[1]\n",
        "    B = img_tensor[2]\n",
        "\n",
        "    def normalize_channel(c):\n",
        "        # Normalisation min-max d'un canal\n",
        "        return (c - c.min()) / (c.max() - c.min() + 1e-6)\n",
        "\n",
        "    # Normalisation indépendante de chaque canal\n",
        "    Rn = normalize_channel(R)\n",
        "    Gn = normalize_channel(G)\n",
        "    Bn = normalize_channel(B)\n",
        "\n",
        "    # Calcul de l'amplitude du canal vert\n",
        "    g_range = G.max() - G.min()\n",
        "\n",
        "    # Renforcement du contraste si le canal vert est peu contrasté\n",
        "    if g_range < 0.15:\n",
        "        gain = 0.3 / (g_range + 1e-6)\n",
        "        Gn = torch.clamp(Gn * gain, 0, 1)\n",
        "\n",
        "    # Reconstruction de l'image normalisée\n",
        "    return torch.stack([Rn, Gn, Bn], dim=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "Bi1j2O4MDSXg"
      },
      "outputs": [],
      "source": [
        "# Transformations appliquées aux images de l'ensemble d'entraînement\n",
        "train_transform = transforms.Compose([\n",
        "    # Conversion de l'image PIL en tenseur PyTorch\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Prétraitement des images \n",
        "    transforms.Lambda(preprocess_retina),\n",
        "\n",
        "    # Normalisation des canaux \n",
        "    transforms.Normalize(mean=[0.5] * 3, std=[0.25] * 3),\n",
        "])\n",
        "\n",
        "# Transformations appliquées aux images de l'ensemble de validation\n",
        "val_transform = transforms.Compose([\n",
        "    # Conversion de l'image PIL en tenseur PyTorch\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Prétraitement aux images \n",
        "    transforms.Lambda(preprocess_retina),\n",
        "\n",
        "    # Normalisation identique à celle de l'entraînement\n",
        "    transforms.Normalize(mean=[0.5] * 3, std=[0.25] * 3),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "vp_uHYenDnia"
      },
      "outputs": [],
      "source": [
        "# Séparation des indices des données en ensembles d'entraînement et de validation\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(images)), \n",
        "    test_size=VAL_SIZE,     \n",
        "    random_state=SEED,       \n",
        "    shuffle=True,            \n",
        "    stratify=labels         \n",
        ")\n",
        "\n",
        "# Extraction des images et labels correspondants aux indices d'entraînement\n",
        "train_images, train_labels = images[train_idx], labels[train_idx]\n",
        "\n",
        "# Extraction des images et labels correspondants aux indices de validation\n",
        "val_images, val_labels = images[val_idx], labels[val_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBXAaQaVDrjV",
        "outputId": "dc93de47-a864-4cbb-b9db-71892b9a8623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train distribution: Counter({np.int64(1): 436, np.int64(0): 413, np.int64(2): 350, np.int64(4): 336, np.int64(3): 330})\n"
          ]
        }
      ],
      "source": [
        "def oversample_class(images, labels, target_class, factor):\n",
        "    # Récupération des indices des images appartenant à la classe cible\n",
        "    idxs = np.where(labels == target_class)[0]\n",
        "\n",
        "    # Listes pour stocker les nouvelles images et leurs labels\n",
        "    new_imgs, new_lbls = [], []\n",
        "\n",
        "    # Boucle sur les images de la classe cible\n",
        "    for idx in idxs:\n",
        "        img = images[idx]\n",
        "\n",
        "        # Génération d'images augmentées pour sur-échantillonner la classe\n",
        "        for _ in range(factor - 1):\n",
        "            aug_img = np.array(augment(Image.fromarray(img)))\n",
        "            new_imgs.append(aug_img)\n",
        "            new_lbls.append(target_class)\n",
        "\n",
        "    return new_imgs, new_lbls\n",
        "\n",
        "\n",
        "# Facteurs de sur-échantillonnage par classe\n",
        "oversample_factor = {0: 1, 1: 4, 2: 2, 3: 2, 4: 6}\n",
        "\n",
        "# Listes pour stocker toutes les nouvelles images générées\n",
        "all_new_imgs, all_new_lbls = [], []\n",
        "\n",
        "# Application du sur-échantillonnage pour chaque classe\n",
        "for cls, factor in oversample_factor.items():\n",
        "    if factor > 1:\n",
        "        imgs_new, lbls_new = oversample_class(\n",
        "            train_images, train_labels, cls, factor\n",
        "        )\n",
        "        all_new_imgs += imgs_new\n",
        "        all_new_lbls += lbls_new\n",
        "\n",
        "# Ajout des images augmentées à l'ensemble d'entraînement\n",
        "if all_new_imgs:\n",
        "    all_new_imgs = np.stack(all_new_imgs)\n",
        "    all_new_lbls = np.array(all_new_lbls)\n",
        "\n",
        "    train_images = np.concatenate([train_images, all_new_imgs])\n",
        "    train_labels = np.concatenate([train_labels, all_new_lbls])\n",
        "\n",
        "# Affichage de la nouvelle distribution des classes\n",
        "print(\"Train distribution:\", Counter(train_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "siDgEox2DtoS"
      },
      "outputs": [],
      "source": [
        "# Création du dataset pour l'ensemble d'entraînement\n",
        "train_dataset = RetinaDataset(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    train_transform\n",
        ")\n",
        "\n",
        "# Création du dataset pour l'ensemble de validation\n",
        "val_dataset = RetinaDataset(\n",
        "    val_images,\n",
        "    val_labels,\n",
        "    val_transform\n",
        ")\n",
        "\n",
        "# DataLoader pour l'entraînement \n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# DataLoader pour la validation \n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beC97O5_DwDk"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Bloc d'extraction de caractéristiques convolutionnelles\n",
        "        self.features = nn.Sequential(\n",
        "            # Première couche convolutionnelle\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Deuxième couche convolutionnelle\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Troisième couche convolutionnelle\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        # Pooling global adaptatif pour réduire la dimension spatiale\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Classifieur final\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.25),             # Régularisation par dropout\n",
        "            nn.Linear(128, num_classes)   # Couche fully-connected\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passage dans les couches convolutionnelles\n",
        "        x = self.features(x)\n",
        "\n",
        "        # Réduction spatiale par global average pooling\n",
        "        x = self.gap(x)\n",
        "\n",
        "        # Mise à plat des caractéristiques\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Prédiction des scores de classes\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "# Initialisation du modèle \n",
        "model = CNN().to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48ke7tA9D0Jo"
      },
      "outputs": [],
      "source": [
        "# Calcul de la distribution des classes dans l'ensemble d'entraînement\n",
        "counter = Counter(train_labels.tolist())\n",
        "\n",
        "weights = torch.tensor(\n",
        "    [1.0 / (counter[c] ** 0.5) for c in range(NUM_CLASSES)],\n",
        "    dtype=torch.float32\n",
        ")\n",
        "\n",
        "weights /= weights.sum()\n",
        "\n",
        "# Définition de la fonction de perte \n",
        "criterion = nn.CrossEntropyLoss(weight=weights.to(DEVICE))\n",
        "\n",
        "# Initialisation de l'optimiseur Adam avec régularisation L2\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqZPsl9BD5pB"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Listes pour stocker toutes les prédictions et les cibles\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    # Boucle sur les mini-lots de l'ensemble d'entraînement\n",
        "    for imgs, targets in loader:\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "        # Remise à zéro des gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Propagation avant\n",
        "        outputs = model(imgs)\n",
        "\n",
        "        # Calcul de la fonction de perte\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Rétropropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Mise à jour des paramètres\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulation de la perte \n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "        # Prédiction des classes \n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        # Sauvegarde des prédictions et des cibles\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_targets.append(targets.detach().cpu().numpy())\n",
        "\n",
        "    # Concaténation des prédictions et des cibles\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "\n",
        "    # Calcul des métriques sur l'ensemble de l'époque\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = accuracy_score(all_targets, all_preds)\n",
        "    epoch_recall_macro = recall_score(all_targets, all_preds, average=\"macro\")\n",
        "    epoch_f1_macro = f1_score(all_targets, all_preds, average=\"macro\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_recall_macro, epoch_f1_macro\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIN-iaZHD9P4"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Listes pour stocker les prédictions et les cibles\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in loader:\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "            # Propagation avant\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            # Calcul de la fonction de perte\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Accumulation de la perte pondérée par la taille du lot\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "            # Prédiction des classes\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            # Sauvegarde des prédictions et des cibles\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Concaténation de toutes les prédictions et cibles\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "\n",
        "    # Calcul des métriques globales\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = accuracy_score(all_targets, all_preds)\n",
        "    epoch_recall_macro = recall_score(all_targets, all_preds, average=\"macro\")\n",
        "    epoch_f1_macro = f1_score(all_targets, all_preds, average=\"macro\")\n",
        "\n",
        "    # Calcul du rappel par classe\n",
        "    class_recalls = recall_score(\n",
        "        all_targets, all_preds,\n",
        "        average=None,\n",
        "        labels=list(range(NUM_CLASSES))\n",
        "    )\n",
        "\n",
        "    # Calcul de la matrice de confusion\n",
        "    cm = confusion_matrix(\n",
        "        all_targets, all_preds,\n",
        "        labels=list(range(NUM_CLASSES))\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"loss\": epoch_loss,\n",
        "        \"acc\": epoch_acc,\n",
        "        \"recall_macro\": epoch_recall_macro,\n",
        "        \"f1_macro\": epoch_f1_macro,\n",
        "        \"class_recalls\": class_recalls,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"y_true\": all_targets,\n",
        "        \"y_pred\": all_preds,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td-OTbOjEL-f",
        "outputId": "1ca1ea8e-5c93-4ee1-ee42-323784a8ede0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [1/20]\n",
            "  Train Loss: 1.5152 | Acc: 0.3223 | Recall: 0.3095 | F1: 0.3015\n",
            "  Val   Loss: 1.3996 | Acc: 0.4321 | Recall: 0.2919 | F1: 0.2546\n",
            "  → Nouveau meilleur ACC: 0.4321\n",
            "  → Nouveau meilleur Recall Macro: 0.2919\n",
            "  → Nouveau meilleur F1 Macro: 0.2546\n",
            "\n",
            "Epoch [2/20]\n",
            "  Train Loss: 1.4223 | Acc: 0.3791 | Recall: 0.3635 | F1: 0.3522\n",
            "  Val   Loss: 1.5572 | Acc: 0.3210 | Recall: 0.3536 | F1: 0.2559\n",
            "  → Nouveau meilleur Recall Macro: 0.3536\n",
            "  → Nouveau meilleur F1 Macro: 0.2559\n",
            "\n",
            "Epoch [3/20]\n",
            "  Train Loss: 1.3736 | Acc: 0.3946 | Recall: 0.3800 | F1: 0.3711\n",
            "  Val   Loss: 1.6164 | Acc: 0.3148 | Recall: 0.3562 | F1: 0.2473\n",
            "  → Nouveau meilleur Recall Macro: 0.3562\n",
            "\n",
            "Epoch [4/20]\n",
            "  Train Loss: 1.3596 | Acc: 0.4129 | Recall: 0.4002 | F1: 0.3859\n",
            "  Val   Loss: 1.3403 | Acc: 0.4691 | Recall: 0.3420 | F1: 0.3401\n",
            "  → Nouveau meilleur ACC: 0.4691\n",
            "  → Nouveau meilleur F1 Macro: 0.3401\n",
            "\n",
            "Epoch [5/20]\n",
            "  Train Loss: 1.3000 | Acc: 0.4638 | Recall: 0.4493 | F1: 0.4391\n",
            "  Val   Loss: 1.4308 | Acc: 0.4321 | Recall: 0.3524 | F1: 0.2956\n",
            "\n",
            "Epoch [6/20]\n",
            "  Train Loss: 1.2759 | Acc: 0.4735 | Recall: 0.4626 | F1: 0.4514\n",
            "  Val   Loss: 1.3767 | Acc: 0.4877 | Recall: 0.3958 | F1: 0.3807\n",
            "  → Nouveau meilleur ACC: 0.4877\n",
            "  → Nouveau meilleur Recall Macro: 0.3958\n",
            "  → Nouveau meilleur F1 Macro: 0.3807\n",
            "\n",
            "Epoch [7/20]\n",
            "  Train Loss: 1.2779 | Acc: 0.4660 | Recall: 0.4532 | F1: 0.4468\n",
            "  Val   Loss: 1.4594 | Acc: 0.4074 | Recall: 0.3756 | F1: 0.3243\n",
            "\n",
            "Epoch [8/20]\n",
            "  Train Loss: 1.2323 | Acc: 0.4879 | Recall: 0.4754 | F1: 0.4656\n",
            "  Val   Loss: 1.4522 | Acc: 0.4198 | Recall: 0.3752 | F1: 0.3698\n",
            "\n",
            "Epoch [9/20]\n",
            "  Train Loss: 1.2325 | Acc: 0.4847 | Recall: 0.4734 | F1: 0.4624\n",
            "  Val   Loss: 1.3795 | Acc: 0.4877 | Recall: 0.3606 | F1: 0.3566\n",
            "\n",
            "Epoch [10/20]\n",
            "  Train Loss: 1.1833 | Acc: 0.5319 | Recall: 0.5188 | F1: 0.5106\n",
            "  Val   Loss: 1.6378 | Acc: 0.3395 | Recall: 0.3419 | F1: 0.2696\n",
            "\n",
            "Epoch [11/20]\n",
            "  Train Loss: 1.1805 | Acc: 0.5303 | Recall: 0.5203 | F1: 0.5132\n",
            "  Val   Loss: 1.3819 | Acc: 0.4321 | Recall: 0.2848 | F1: 0.2548\n",
            "\n",
            "Epoch [12/20]\n",
            "  Train Loss: 1.0970 | Acc: 0.5769 | Recall: 0.5669 | F1: 0.5597\n",
            "  Val   Loss: 1.3976 | Acc: 0.4753 | Recall: 0.4285 | F1: 0.3769\n",
            "  → Nouveau meilleur Recall Macro: 0.4285\n",
            "\n",
            "Epoch [13/20]\n",
            "  Train Loss: 1.0750 | Acc: 0.5753 | Recall: 0.5684 | F1: 0.5628\n",
            "  Val   Loss: 1.3987 | Acc: 0.4383 | Recall: 0.3907 | F1: 0.3787\n",
            "\n",
            "Epoch [14/20]\n",
            "  Train Loss: 1.0445 | Acc: 0.6145 | Recall: 0.6069 | F1: 0.6010\n",
            "  Val   Loss: 1.3509 | Acc: 0.4383 | Recall: 0.3756 | F1: 0.3393\n",
            "\n",
            "Epoch [15/20]\n",
            "  Train Loss: 1.0407 | Acc: 0.6048 | Recall: 0.6000 | F1: 0.5977\n",
            "  Val   Loss: 1.4120 | Acc: 0.4383 | Recall: 0.3330 | F1: 0.2988\n",
            "\n",
            "Epoch [16/20]\n",
            "  Train Loss: 1.0190 | Acc: 0.6241 | Recall: 0.6174 | F1: 0.6109\n",
            "  Val   Loss: 1.4471 | Acc: 0.4198 | Recall: 0.3622 | F1: 0.3135\n",
            "\n",
            "Epoch [17/20]\n",
            "  Train Loss: 1.0117 | Acc: 0.6166 | Recall: 0.6110 | F1: 0.6062\n",
            "  Val   Loss: 1.4535 | Acc: 0.4568 | Recall: 0.3830 | F1: 0.3496\n",
            "\n",
            "Epoch [18/20]\n",
            "  Train Loss: 0.9772 | Acc: 0.6408 | Recall: 0.6340 | F1: 0.6306\n",
            "  Val   Loss: 1.3068 | Acc: 0.4877 | Recall: 0.3421 | F1: 0.3382\n",
            "\n",
            "Epoch [19/20]\n",
            "  Train Loss: 0.9584 | Acc: 0.6456 | Recall: 0.6390 | F1: 0.6364\n",
            "  Val   Loss: 1.3375 | Acc: 0.4938 | Recall: 0.3695 | F1: 0.3663\n",
            "  → Nouveau meilleur ACC: 0.4938\n",
            "\n",
            "Epoch [20/20]\n",
            "  Train Loss: 0.9492 | Acc: 0.6568 | Recall: 0.6514 | F1: 0.6464\n",
            "  Val   Loss: 1.3398 | Acc: 0.5309 | Recall: 0.4711 | F1: 0.4532\n",
            "  → Nouveau meilleur ACC: 0.5309\n",
            "  → Nouveau meilleur Recall Macro: 0.4711\n",
            "  → Nouveau meilleur F1 Macro: 0.4532\n"
          ]
        }
      ],
      "source": [
        "# Réduit le learning rate lorsque la métrique surveillée stagne\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='max',        # On cherche à maximiser la métrique\n",
        "    factor=0.5,        # Facteur de réduction du learning rate\n",
        "    patience=4         # Nombre d'époques sans amélioration tolérées\n",
        ")\n",
        "\n",
        "# Variables pour suivre les meilleures performances\n",
        "best_acc = best_recall = best_f1 = 0.0\n",
        "\n",
        "# Boucle principale d'entraînement\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "    # Entraînement sur une époque\n",
        "    train_loss, train_acc, train_recall, train_f1 = train_one_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        DEVICE\n",
        "    )\n",
        "\n",
        "    # Évaluation sur l'ensemble de validation\n",
        "    val_metrics = evaluate(model, val_loader, criterion, DEVICE)\n",
        "\n",
        "    # Récupération des métriques de validation\n",
        "    val_loss = val_metrics[\"loss\"]\n",
        "    val_acc = val_metrics[\"acc\"]\n",
        "    val_recall = val_metrics[\"recall_macro\"]\n",
        "    val_f1 = val_metrics[\"f1_macro\"]\n",
        "\n",
        "    # Affichage des résultats de l'époque courante\n",
        "    print(f\"\\nEpoch [{epoch}/{NUM_EPOCHS}]\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Acc: {train_acc:.4f} | \"\n",
        "          f\"Recall: {train_recall:.4f} | \"\n",
        "          f\"F1: {train_f1:.4f}\")\n",
        "    print(f\"  Val   Loss: {val_loss:.4f} | \"\n",
        "          f\"Acc: {val_acc:.4f} | \"\n",
        "          f\"Recall: {val_recall:.4f} | \"\n",
        "          f\"F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Sauvegarde du modèle si une métrique s'améliore\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model_1.pth\")\n",
        "        print(f\" Nouveau meilleur Accuracy: {best_acc:.4f}\")\n",
        "\n",
        "    if val_recall > best_recall:\n",
        "        best_recall = val_recall\n",
        "        torch.save(model.state_dict(), \"best_model_1.pth\")\n",
        "        print(f\" Nouveau meilleur Recall Macro: {best_recall:.4f}\")\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        torch.save(model.state_dict(), \"best_model_1.pth\")\n",
        "        print(f\"  Nouveau meilleur F1 Macro: {best_f1:.4f}\")\n",
        "\n",
        "    # Mise à jour du learning rate en fonction du recall macro\n",
        "    scheduler.step(val_recall)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc3MZ69DEO6F",
        "outputId": "026adc53-f48f-4907-93bf-97567f715657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.3398\n",
            "Accuracy: 0.5309\n",
            "Macro Recall: 0.4711\n",
            "Macro F1: 0.4532\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6753    0.7123    0.6933        73\n",
            "           1     0.5455    0.3158    0.4000        19\n",
            "           2     0.4000    0.4516    0.4242        31\n",
            "           3     0.3810    0.2759    0.3200        29\n",
            "           4     0.3333    0.6000    0.4286        10\n",
            "\n",
            "    accuracy                         0.5309       162\n",
            "   macro avg     0.4670    0.4711    0.4532       162\n",
            "weighted avg     0.5336    0.5309    0.5243       162\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAt0AAAJOCAYAAABrxbsfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAToBJREFUeJzt3Q18zXX/x/HPb2wzG5u7trmZe+YmioSE3GThkrukkiglQm5S7KpEaK5u3HQxlXshUUlKJIV0ISaV25Jybwwbwza28398v9d/u3YYttnvd3bO7/W8Hr/Lzu+cnfM937Otz3mfz+/7MxwOh0MAAAAAmMbLvLsGAAAAQNENAAAAWICkGwAAADAZRTcAAABgMopuAAAAwGQU3QAAAIDJKLoBAAAAk1F0AwAAACaj6AYAAABMRtENIEdGjx4thmHki1mbO3euHsvff/8t7m7VqlVyxx13SKFChfRzio+Pz9P796S5AgB3RNEN5FPpRZLaNm7ceM31DodDypUrp6//xz/+kavHeOONN+Tzzz/Pg9HiVpw+fVoefvhh8fPzk2nTpsmHH34o/v7+TKqIxMbGyvDhwyU8PFwKFy6s56V+/foybtw4pzcm9913n/5d6NChwzXzpt5oqOvefvvtjH3r1q3L+P2KiYm55nt69+4tAQEBvAYA8gxFN5DPqeRz0aJF1+xfv369HDlyRHx9fXN937kpul955RW5dOlSrh8T19q6daucP39exo4dK3369JHHH39cvL2983SqevbsqV+38uXLu9W81K5dW78Radq0qUycOFHeeecdufPOO2XChAn6jcrVvvzyyyyL6Jt9egMAZito+iMAuCXt2rWTpUuXyrvvvisFC/7vV1YV4irxi4uLs2SGL1y4oFNGNYbM48CtO3nypP43KCjItOksUKCA3tyFSrE7d+6sx/zzzz/rpDuz8ePHy4wZM5z2hYWF6TcvY8aMkS+++CJbj6NaelShvn37dqlXr16ePgcAyIykG8jnHn30Ud1+sGbNmox9KSkp8sknn8hjjz2W5feoj9HvueceKVGihG5ZUMW5un1m6mN1VUjPmzcv42N29ZF65r7t3bt368coVqyY3HvvvU7XXW3BggVy99136xYAdftmzZrJN99843Sbr7/+WieWqngvUqSItG/fXnbt2pWteVC3a9mypX4+ZcuW1e0FaWlpWd72Vh5HFXtDhw6VChUq6E8R1GM98cQTTm9uVJGsEung4GD9SUTdunX1PF6vpeGDDz6QypUr6/tr0KCBTnAzt0X06tVLf62uy/w6qDGkf52Z+h61Zfbvf/9batWqlTH/d911l9MnJNfr6Y6Ojtbfp8ZWunRpGTBgwDX95OqxVOKsfh5atGihH6NMmTLy5ptvilnef/99OXr0qE63ry64FTX36lOXzNRrrV67FStW6CI6OwYNGqTni7QbgNkouoF8ThVejRs3lo8++sipqExISJBHHnkky++ZMmWK/gj+9ddf1y0kKpnu1q2bfPXVVxm3UX3DqtBSxan6Wm3PPvus0/2o77l48aK+j2eeeea6Y1TJompfUC0R6jHVZdVv/t133zk9nip+VZ/sv/71L3n11Vd1EaeK+Zsd3HfixAld7O3YsUNGjhwpQ4YMkfnz5+vnebVbeZzExEQ9H6qAbdOmjb7/fv36yd69e3Urj6JaNFQRqh6nR48e8tZbb0lgYKAujrMajyp81W3U3Ko3CmoMXbp0kcuXL+vrX375Zenbt6/+Ws1dVq/DzajE9/nnn5eaNWvK5MmT9fyrBHfLli03/D5VaKoiWxXbqm2ja9euuthVzz19fOnOnj0rDzzwgH6DoW6rCuERI0bon0UzqKRavcF66KGHcvR9gwcPzlERXbRo0RwX6gCQKw4A+dKcOXMc6ld069atjqlTpzqKFCniuHjxor6uW7dujhYtWuivy5cv72jfvr3T96bfLl1KSoqjdu3ajpYtWzrt9/f3d/Tq1euax37ttdf0Yz/66KPXvS7dH3/84fDy8nJ07tzZkZqa6nTbtLQ0/e/58+cdQUFBjmeeecbp+hMnTjgCAwOv2X+1IUOG6MfcsmVLxr6TJ0/q71X7//rrrzx5nFGjRun7++yzz665Lv25TJ48Wd9mwYIFTvPbuHFjR0BAgOPcuXN6nxqTul2JEiUcZ86cybjt8uXL9f4VK1Zk+Vpnpl7brF6f5s2b6y1dx44dHbVq1brhc0t/jPS5UvPn4+PjaNOmjdPrpn7W1O1mz57t9Hhq3/z58zP2JScnO0JCQhxdu3Z1mKFYsWKOunXrZvv2aozpczBmzBg93piYGKfX4q233sq4/ffff6/3LV261BEfH68f78EHH8y4Xs27+v0AgLxC0g24AXXAmEpYVe+p6llV/16vtURRCWHmhFKl4irBzWmSp1Lem1EHYqo2j1GjRomXl/OflPQ2FNUao1oWVKuMatNI31S/bsOGDeX777+/4WOsXLlSGjVqpNtX0pUqVUonzZnd6uN8+umnOslVvcRXS38uaiwhISH6MdKphF8lzSopVwe4Zta9e3edvKZTr4Ny4MABySuqF1wl8ZnbVm7m22+/1W1K6lODzK+b+kRDpb+ZPxVR1CcH6gDPdD4+Pvr1yMvnkdm5c+d0u0hupKfdKvHPDvVJhZoHla6r/nEAMANHQwFuQBWYrVu31q0Kqt0jNTX1hh+7q6JctTKodozk5OSM/TldX7tixYo3vc2ff/6pizbV2nA9f/zxh/5X9WRnRRV5N3Lw4EFdNF+tevXqefo46rmoFoubjaVq1arXvMGoUaNGxvVXH9yXWXoBrt4M5RXV5qGKaFUEV6lSRbeHqDdlTZo0ueHzyGoOVTFdqVKla56H6m2/+udHPZdff/31hmM7c+aMLu6zot68XI96rdQbzNxIL6Jfe+01XURnftNzo0J90qRJui1l+fLluXpcALgRim7ATagiSqWQqr+5bdu2113p4ocffpAHH3xQH8ioDpILDQ3VSeycOXOyXHrwRjIn5rci/YBH1a+cVaGVV6uhWPU4OXG9FUPUOus3c703SepNV+b7VQX/vn379JstdZIdldir1159+pDdtNes56H6169O/7PzvapnXL1pVAW7eiOQU+lFtHr+qs89u4W6KrpJuwGYgaIbcBOq5UEdYLd582b5+OOPr3s7VXCpFTVWr17ttIa3KrqvlhdnllSrcqhiVx2sqA7eu95tlNtuu00n9jml1pZOT7EzU4VmXj6O+v6dO3fedCwq3VXPOXParQ62TL8+r6iENqszU6oUWqXRmamVWlQri9pUoaqKXbWsXmRkpP55yOp5pM9h5vtS3/vXX3/lav6yog66zE2qr05ys2nTJv3znLmVJ7syF9Hpq8PcjLp9+oGoZi7fCMCe6OkG3ITqqZ0+fbouIrI6617mRFIV0yoNTadWzMjqJDiqULvV04136tRJF59q5Y2rl/BLTzIjIiJ0u4BaBeXqVTGUU6dO3XStcvVm46effnL6noULFzrd7lYfR7WW/PLLL7Js2bJrrkt/Lmos6tOGzG98rly5olc8Ua9R8+bNJa+oNwHqeWduz1Bp9uHDh51up5aUzEwlw6rdR405q3lQVFGtbqfWf8+cOM+aNUsfA6BWgMkLarlK9VhZbTc7nkB9SvPCCy/I77//fs31atlG1UJ1syJaFc/qZzMnhbpqL1EpOwDkJZJuwI1kJ7FTxZJa21gt76ZaUlRxos7op3p9r+6/VQWR6gVWt1fLxqke7qx6p29E3a9a9k6dTVEdJKgSVpWwq4P61H1GRUXpQli9YVDLCqoTkKilDlWf+qFDh/QBe6r3eOrUqdd9jJdeekm3jKjnpNoG1JsFtfZ1euqc7lYf58UXX9TrmaulEp966ik9P6onWR1g99577+mDLNXyfmpZPbVEoDrzoVrSUX3Pjz/+qFPS3B78l5Wnn35a37d63upgWtVzrtZDT0/006kebtVOo56fWr96z549+nmqn4XrjUfNi0rBVaqr7l+1JKnUW7WlqPXCMx806Qoq5VdvftSbHPUJihqPej0UdUCwWkJTLaV5syJa/bzkpMUmvS1FvflSP2cAkGfybB0UAHnqesvIXS2rJQNnzZrlqFq1qsPX19cRHh6u7+vqpf6UvXv3Opo1a+bw8/PT16UvT5d+21OnTl3zeFndj6KWmLvzzjv1Y6rl19QSbmvWrHG6jVqmLSIiQi/fV6hQIUflypUdvXv3dmzbtu2m8/Hrr7/q+1TfV6ZMGcfYsWP188y8DF5ePM7p06cdAwcO1I+hltQrW7asnpe4uLiM28TGxjqefPJJR8mSJfVtbr/9dj3HmWW1TF06tV/NY3Ze63feeUePRc1rkyZN9HO4esnA999/X7+OanlCdTv1fF988UVHQkLCNY9x9VypJQLVz4i3t7cjODjY0b9/f8fZs2evuxxfZmpe1M+fmY4dO+YYOnSoo1q1avq1LFy4sKN+/fqO8ePHOz2/641RPZf0pSWvt2Tg9X7GWTIQQF4y1P/lXQkPAAAA4Gr0dAMAAAAmo+gGAAAATEbRDQAAAJiMohsAAAAwGUU3AAAAYDKKbgAAAMBkFN0AAACAyTzyjJR+dw509RCQx45snMycepD4i1mfmhzuqaAX+Y0nKVjAcPUQkMfKBPnYrk679PP1zz7sKvylBAAAAEzmkUk3AAAA8inDnpmvPZ81AAAAYCGSbgAAAFjHsOdxAyTdAAAAgMlIugEAAGAdw56Zrz2fNQAAAGAhkm4AAABYx6CnGwAAAIAJSLoBAABgHcOe3c32fNYAAACAhUi6AQAAYB2Dnm4AAAAAJiDpBgAAgHUMe3Y32/NZAwAAACIyevRoMQzDaQsPD8+Ym6SkJBkwYICUKFFCAgICpGvXrhIbG5vjuaPoBgAAgLU93YbJWw7VqlVLjh8/nrFt3Lgx47qhQ4fKihUrZOnSpbJ+/Xo5duyYdOnSJcePQXsJAAAAbK1gwYISEhJyzf6EhASZNWuWLFq0SFq2bKn3zZkzR2rUqCGbN2+WRo0aZfsxSLoBAABgbU+3YfKWQ3/88YeULl1aKlWqJD169JBDhw7p/TExMXL58mVp3bp1xm1V60lYWJhs2rQpR49B0g0AAACPkpycrLfMfH199Xa1hg0byty5c6V69eq6tWTMmDHStGlT2blzp5w4cUJ8fHwkKCjI6XuCg4P1dTlB0g0AAACP6umOioqSwMBAp03ty0rbtm2lW7duUqdOHYmIiJCVK1dKfHy8LFmyJE+fNkU3AAAAPEpkZKTux868qX3ZoVLtatWqyf79+3Wfd0pKii7CM1Orl2TVA34jFN0AAADwqJ5uX19fKVq0qNOWVWtJVhITE+XPP/+U0NBQqV+/vnh7e8vatWszrt+3b5/u+W7cuHGOnjY93QAAALCt4cOHS4cOHaR8+fJ6OcDXXntNChQoII8++qhuS+nTp48MGzZMihcvrov3QYMG6YI7JyuXKBTdAAAAsI6R83W0zXTkyBFdYJ8+fVpKlSol9957r14OUH2tTJo0Sby8vPRJcdTBmarvOzo6OsePYzgcDod4GL87B7p6CMhjRzZOZk49SPzFy64eAvJQQS86FT1JwQL5qyDCrSsT5JOvptGv6SjTH+PSD69LfkPSDQAAAOsY9nyjbs9nDQAAAFiIpBsAAADWMeyZ+drzWQMAAAAWIukGAACAdbzsebAuSTcAAABgMpJuAAAAWMewZ+Zrz2cNAAAAWIikGwAAALY9I6VVSLoBAAAAk5F0AwAAwDqGPTNfez5rAAAAwEIk3QAAALCOQU83AAAAABOQdAMAAMA6hj27m+35rAEAAAALkXQDAADAOgY93QAAAABMQNINAAAA6xj27G6m6M7nXn62nbzSr53Tvn1/nZA7uoyTYkULy6v920urRuFSLqSYxJ1NlBXrfpUx0V/KucQkl40Z2ffZ0sWybOnHcvz4UX25YqUq8lTf/tK4SVOm0U31fqitnDxx/Jr97Ts/LANe+KdLxoRbE3cqVmZOmyxbN2+U5KQkKV22nAx/eaxUq1GLqXUzc2dEy/yZ0532lStfQeYtWeGyMcE+KLrdwK79x6R9v39nXL6Smqb/DS0VqLfISctkz4ETEhZaXP798iN632MvznLhiJFdt90WLP2fHyrlwsqLw+GQlSuWy4ihA2XuR59KpcpVmEg3NGXGQklN++/vqHLwwH55eWg/adrifpeOC7lz/tw5GfpsL6lbr4GMnxgtgUHF5OjhQxJQpChT6qYqVKoib0+dkXG5QIECLh2PLRn27Omm6HYDqsiOPX3+mv27/zwujw6fmXH5ryNxMnrqCpk9/gkpUMBLUv+/OEf+dW/zFk6X+w0cLMs+WSy7fvuFottNBRYr7nR56YLZElqmnNx+510uGxNyb8mC2VIqOFiGvzI2Y19o6bJMqRtTRXbxEiVdPQzYkEuL7ri4OJk9e7Zs2rRJTpw4ofeFhITIPffcI71795ZSpUq5cnj5RpWwUnLgm/GSlHxZtvz6l4z69xdy+MTZLG9btEghOXchiYLbDaWmpsp3366WpEuXpHaduq4eDvLA5cuX5ftvVkrn7o+LYdNkx91t2rhO6je8R8a+/IL8+vM2KVkqWDp0eVjadXzI1UNDLqlPKrq1byk+Pj5S8/a68vRzQyQ4JJT5tJJBT7eltm7dKhEREVK4cGFp3bq1VKtWTe+PjY2Vd999VyZMmCCrV6+Wu+6ydzq0deff0nfUAvn9YKyElAyUl59tK9/OHir1HxoviReTnW5bIshfIp9pK7M//Y/Lxouc+/OP36Vv78ckJSVF/PwKS9Q77+rebri/TRu+k8TE89K63YOuHgpy6fixI/LlsiXS9ZGe8ugTT8u+PbsketK/pKC3t7Rp15F5dTM1at0uL40aK+XCKsiZ03Eyb+Z0GfxsL5m9aJkU9vd39fDg4QyHaiR1gUaNGkndunXlvffeuyYBUkPq16+f/PrrrzoFv5Hk5GS9ZXZb0xFieHlmj1ZggJ/sW/m6jJj4mcz7/H9zU8S/kHw1faCcOXdBHhryvly54lmtJUc2ThZPdflyisQePy6JiYny/dpvZMWyT2XazLkeXXjHX7wsdvDKsP5SsKC3jH7zXfFkBb08N7Vq16yeVAuvJZM/+DBj37SJE+T3PTtlyowF4okKFrDPpzKJ58/Jox0jpP+QF6Xdg13EU5UJ8pH8xK+9+X8TL331vOQ3LvtL+csvv8jQoUOz/MhV7VPX7dix46b3ExUVJYGBgU7bldgY8VQJiZdk/6GTUrnc/1pvAgr7yhfTnpPzF5Ok+7AZHldwezpvbx8pG1ZewmvWkv6DhkqVatVlySLP/I+5ncSeOCY7tm2RiA6dXT0U3ILiJUpJWMVKTvvCKlSUk7H/bYmEe1MHxKq/v6rlBPDYolv1bv/000/XvV5dFxwcfNP7iYyMlISEBKetYHB98VT+fj5SsWxJORGXkJFwfzl9oKRcTtUJd3LKFVcPEbcoLS1Np99wb2u+Wq4Pqry7Mcs/urNade6QI4f+dtp35PBBeoA9xKWLF+XY0cNSoiTHkFne022YvOVDLjuQcvjw4dK3b1+JiYmRVq1aZRTYqqd77dq1MmPGDHn77bdvej++vr56y8yTWkuihnaWrzb8JoeOnZHStwXKK/3a6+XIlqyK+W/BHT1A/Ar5yJMvz5Oi/oX0ppw6myhpaS7pHEIOTP/3JGl0T1MJCQ2VixcuyDervpKfY7bKpGkfMI9u/sZpzcovpPUDHaRAQRaJcmdduveUIc8+IR/NmyHNWkXIvt2/ycrln8iQEa+5emjIhelT3pZ7mjaX4JDSEhd3SubNmCZeXgWkZZu2zCdM57L/GgwYMEBKliwpkyZNkujoaL1yQ/pSPvXr15e5c+fKww8/LHZXJjhI5kc9KcUDC+uT3/xnxwFp/sQ7+uum9avK3XUq6tvtXjHa6fuqtxslh46fcdGokV1nz5yRsaMi5XTcKfEPKCJVqlbTBffdje5hEt3Yjm2b5VTscbm/fSdXDwW3qHrN2vLahEkye/oUWTDnfQkJLSP9B78krSLaM7duKO5krIx7dYScS4jXa67fXreeTJ21UIKuWuoTJjPyZxLtsQdSXr2sllo+UFGFuLe39y3dn9+dA/NoZMgvPPlASjuyy4GUduHJB1LakZ0OpLSLfHcgZYdo0x/j0ornJL/JF597qiI7NJQ1MgEAADyeYc83dvmi6AYAAIBNGPb8dMyezxoAAACwEEk3AAAArGPYs72EpBsAAAAwGUk3AAAArGPYM/O157MGAAAALETSDQAAAOsY9HQDAAAAMAFJNwAAACxjkHQDAAAAMANJNwAAACxjkHQDAAAAMANJNwAAAKxj2HOyWacbAAAAMBlJNwAAACxj0NMNAAAAwAwk3QAAALCMQdINAAAAwAwk3QAAALCMQdINAAAAwAwk3QAAALCMQdINAAAAwAwk3QAAALCOYc/J5oyUAAAAgMlIugEAAGAZg55uAAAAAGYg6QYAAIBlDJJuAAAAAGYg6QYAAIBlDJJuAAAAAGYg6QYAAIBlDJJuAAAAAGYg6QYAAIB1DHtONmekBAAAAExG0g0AAADLGPR0AwAAADADSTcAAAAsY5B0AwAAADADSTcAAAAsY5B0AwAAADADSTcAAACsY9hzslmnGwAAADAZSTcAAAAsY9DTDQAAAMAMHpl07/9+oquHgDyWmJTKnHqQoMLerh4C8tCVVAfz6UECCnlkaYB8xCDpBgAAAGAG3s4CAADAMgZJNwAAAAAzkHQDAADAMgZJNwAAAAAzkHQDAADAOoY9J5szUgIAAAAmI+kGAACAZQx6ugEAAACYgaQbAAAAljFIugEAAAB7mzBhgn5jMGTIkIx9SUlJMmDAAClRooQEBARI165dJTY2Nkf3y4GUAAAAsIxhGKZvubV161Z5//33pU6dOk77hw4dKitWrJClS5fK+vXr5dixY9KlS5cc3TdFNwAAAGwvMTFRevToITNmzJBixYplzEdCQoLMmjVLJk6cKC1btpT69evLnDlz5D//+Y9s3ryZohsAAAD5kGHBlguqfaR9+/bSunVrp/0xMTFy+fJlp/3h4eESFhYmmzZtyvb9cyAlAAAAPEpycrLeMvP19dVbVhYvXizbt2/X7SVXO3HihPj4+EhQUJDT/uDgYH1ddtFeAgAAAI/q6Y6KipLAwECnTe3LyuHDh2Xw4MGycOFCKVSokGnPm6QbAAAAHiUyMlKGDRvmtO96KbdqHzl58qTUq1cvY19qaqps2LBBpk6dKqtXr5aUlBSJj493SrvV6iUhISHZHhNFNwAAADxqnW7fG7SSXK1Vq1by22+/Oe178skndd/2iBEjpFy5cuLt7S1r167VSwUq+/btk0OHDknjxo2zPSaKbgAAANhWkSJFpHbt2k77/P399Zrc6fv79Omjk/PixYtL0aJFZdCgQbrgbtSoUbYfh6IbAAAAljHc8IyUkyZNEi8vL510qwM0IyIiJDo6Okf3YTgcDod4mKPxKa4eAvLYlVSP+zG1tYBCBVw9BOQhfj89S0Ah8jhP4++Tv4rcikO+Mv0x/prcXvIbVi8BAAAATMbbWQAAAFjHsOdkk3QDAAAAJiPpBgAAgGUMNzyQMi+QdAMAAAAmI+kGAACAZQySbgAAAABmIOkGAACAZQx7tnTT0w0AAACYjaQbAAAAljFsGnWzegkAAABgMpJuAAAAWMawZ9BN0g0AAACYjaQbAAAAljFsGnXT0w0AAACYjKQbAAAAljHsGXSTdAMAAABmI+kGAACAZby87Bl109MNAAAAmIykGwAAAJYx7Bl0k3QDAAAAZiPpBgAAgGUMm0bd9HQDAAAAJiPpdjNzZ0TL/JnTnfaVK19B5i1Z4bIx4dbEnYqVmdMmy9bNGyU5KUlKly0nw18eK9Vq1GJq3dBnSxfLsqUfy/HjR/XlipWqyFN9+0vjJk1dPTTkgQVzZ8oH0ybLQ488Ls+/MJI5dUMx27bK/LmzZM/uXRJ36pS8M3mqtGjV2tXDshXDnkE3Rbc7qlCpirw9dUbG5QIFCrh0PMi98+fOydBne0ndeg1k/MRoCQwqJkcPH5KAIkWZVjd1223B0v/5oVIurLw4HA5ZuWK5jBg6UOZ+9KlUqlzF1cPDLdiz6zf5YtlSqVy1GvPoxpIuXZJq1cKlY+euMnzIIFcPBzZC0u2GVJFdvERJVw8DeWDJgtlSKjhYhr8yNmNfaOmyzK0bu7d5C6fL/QYOlmWfLJZdv/1C0e3GLl68KGNHjZSX/jla5s9+39XDwS1o0rSZ3uA6hk2jbnq63ZBKQru1byk9Oj8g40eNkNgTx109JOTSpo3rpGp4LRn78gvSrV1z6d/rYVm5/BPm00OkpqbKmtUrdbJWu05dVw8Ht2DSm+OkcZNmclfDxswjAM9Lug8fPiyvvfaazJ4929VDyTdq1LpdXho1VsqFVZAzp+Nk3szpMvjZXjJ70TIp7O/v6uEhh44fOyJfLlsiXR/pKY8+8bTs27NLoif9Swp6e0ubdh2ZTzf15x+/S9/ej0lKSor4+RWWqHfe1b3dcE9rv1kpv+/dIx/MW+zqoQAewbBp0p2vi+4zZ87IvHnzblh0Jycn6815nyG+vr7iiRre87+DsSpXra6L8Ec7Rsi6taul3YNdXDo25JwjLU2qhdeSp/oN1perVK8hfx/YL18tW0rR7cbCKlSQeR99KomJifL92m9k3Kh/yrSZcym83ZD6JPHddybIxKkzPPa/KwBsUHR/8cUXN7z+wIEDN72PqKgoGTNmjNO+oSNekRdGvip2oA64KxtWXrecwP0UL1FKwipWctoXVqGibFz3rcvGhFvn7e2jfy+V8Jq1ZM+unbJk0QIZ8cpoptfN/L53t5w9c0ae7vmwU9vQLz/HyLKlH8m3P27nYHYghwx7Bt2uLbo7deqkP2JQR/jn9iOIyMhIGTZsmNO+uEv2eTUvXbwox44elvvbdnD1UJALtercIUcO/e2078jhgxIcEsp8epC0tDS5fDnF1cNALtRv0EjmfrTMad+E11/Rb44fe6IPBTcA9yi6Q0NDJTo6Wjp2zLp3dceOHVK/fv0b3of6uO/qj/zOp3nuf9ymT3lb7mnaXIJDSktc3CmZN2OaeHkVkJZt2rp6aMiFLt17ypBnn5CP5s2QZq0iZN/u3/SBlENGvMZ8uqnp/54kje5pKiGhoXLxwgX5ZtVX8nPMVpk07QNXDw25oI6VqVSlqtO+Qn5+UjQw6Jr9cA8XL16Qw4f+9+nw0aNHZN/ePVI0MFBCQ0u7dGx2Ydg06nZp0a0K6piYmOsW3TdLwe0o7mSsjHt1hJxLiNdrOt9et55MnbVQgooVd/XQkAvVa9aW1yZMktnTp8iCOe9LSGgZ6T/4JWkV0Z75dFOqFWHsqEg5HXdK/AOKSJWq1XTBfXeje1w9NAAisnvXTun7VK+MuZj41gT9b4cHO8mY8f/9GjCD4XBhVfvDDz/IhQsX5IEHHsjyenXdtm3bpHnz5jm636Pxnpt029WVVN58eZKAQpzQyZPw++lZAgrl6zUWkAv+PvkrWa73+nemP8b2US0lv3Hpb1bTpjc+LbK/v3+OC24AAAAgv+HtLAAAACxj2LSnmzNSAgAAACYj6QYAAIBlDHsG3STdAAAAgNlIugEAAGAZw6ZRNz3dAAAAgMlIugEAAGAZw55BN0k3AAAAYDaSbgAAAFjGsGnUTU83AAAAYDKSbgAAAFjGsGfQTdINAAAAmI2kGwAAAJYxbBp109MNAAAAmIykGwAAAJYx7Bl0k3QDAAAAZiPpBgAAgGUMm0bd9HQDAAAAJiPpBgAAgGUMewbdJN0AAACA2Ui6AQAAYBnDplE3Pd0AAACAyUi6AQAAYBmDpBsAAACAGUi6AQAAYBnDni3d9HQDAAAAZiPpBgAAgGUMm0bdrF4CAAAAmIykGwAAAJYx7Bl0k3QDAAAAZiPpBgAAgGUMm0bdFN0AAACwjGHPmpv2EgAAAMBsJN0AAACwjJdNo26WDAQAAABMRtINAAAAyxj2DLpJugEAAACzkXQDAADAMoZNo256ugEAAACTkXQDAADAMl72DLpJugEAAACzkXQDAADAMgY93QAAAADMQNINAAAAyxg27en2yKI74eJlVw8Beax8ycLMqQeJ+fusq4eAPFS3XBDz6UHi+W+ox/H38XH1EOCpRTcAAADyJ0PsGXWzTjcAAABgMpJuAAAAWMbLnkE3STcAAABgNpJuAAAAWMaw6fIl9HQDAAAAJqPoBgAAgGUMw/wtJ6ZPny516tSRokWL6q1x48by9ddfZ1yflJQkAwYMkBIlSkhAQIB07dpVYmNjc/y8KboBAABgW2XLlpUJEyZITEyMbNu2TVq2bCkdO3aUXbt26euHDh0qK1askKVLl8r69evl2LFj0qVLlxw/Dj3dAAAAsIxXPuvp7tChg9Pl8ePH6/R78+bNuiCfNWuWLFq0SBfjypw5c6RGjRr6+kaNGmX7cUi6AQAAABFJTU2VxYsXy4ULF3SbiUq/L1++LK1bt86Yn/DwcAkLC5NNmzblaM5IugEAAGAZw4KgOzk5WW+Z+fr66i0rv/32my6yVf+26ttetmyZ1KxZU3bs2CE+Pj4SFBTkdPvg4GA5ceJEjsZE0g0AAACPEhUVJYGBgU6b2nc91atX1wX2li1bpH///tKrVy/ZvXt3no6JpBsAAAAetU53ZGSkDBs2zGnf9VJuRaXZVapU0V/Xr19ftm7dKlOmTJHu3btLSkqKxMfHO6XdavWSkJCQHI2JpBsAAAAexdfXN2MJwPTtRkX31dLS0nR7iirAvb29Ze3atRnX7du3Tw4dOqTbUXKCpBsAAACWMfLX4iU6FW/btq0+OPL8+fN6pZJ169bJ6tWrdVtKnz59dGpevHhxXbwPGjRIF9w5WblEoegGAACAbZ08eVKeeOIJOX78uC6y1YlyVMF9//336+snTZokXl5e+qQ4Kv2OiIiQ6OjoHD+O4XA4HOJhdh+74OohII+VL1mYOfUgMX+fdfUQkIfqlnM+qh/uLTH5iquHgDxWJsgnX81p93k/m/4YH/e6U/IberoBAAAAk9FeAgAAAMsYNp1rkm4AAADAZCTdAAAA8Kh1uvMjkm4AAADAZCTdAAAAsIyXPYNukm4AAADAbCTdAAAAsIxBTzcAAAAAM5B0AwAAwDIGPd0AAAAAzEDSDQAAAMsYNo26WacbAAAAMBlJNwAAACzjZc+gm6QbAAAAMBtJNwAAACxj0NMNAAAAwAwk3QAAALCMYdO5ZvUSAAAAwGQk3QAAALCMFz3dAAAAAFyadA8bNizbdzpx4sTcjgcAAAAezLBpU3e2i+6ff/45W7ez6zIwAAAAwC0X3d9//312bwoAAABkybBpQHtLq5fs379fVq9eLZcuXdKXHQ5HXo0LAAAAsHfRffr0aWnVqpVUq1ZN2rVrJ8ePH9f7+/TpIy+88EJejxEAAAAewjDM3zym6B46dKh4e3vLoUOHpHDhwhn7u3fvLqtWrcrL8QEAAAD2XKf7m2++0W0lZcuWddpftWpVOXjwYF6NDddx6eIFWTQ7WrZs/F4Szp6VilWrS5+BL0rV8FrMmRuK2bZV5s+dJXt275K4U6fknclTpUWr1q4eFrLp950/y+rPFsrBP/dJwpk4ee6fE+TOxs2zvO2H0/4lG1Z9Lt2fHiytOz7CHLuBubM+kO/XrpGDfx8QX99CcnvdO2XQkBekfIWKrh4acmHujGiZP3O6075y5SvIvCUrmE8LeeXXKDo/Ft0XLlxwSrjTnTlzRnx9ffNiXLiBaW+9Lof++lMGR46V4iVLyfo1K2X08P7y7pxPpESp25g7N5N06ZJUqxYuHTt3leFDBrl6OMih5KQkKVuxqjS5/x8y/Y3I695u+6Z1cmDfLgkqXpI5diPbY7ZKt+6PSY1atSU1NVWm/3uSDOrfRz7+7Evx87v2v4PI/ypUqiJvT52RcblAgQIuHQ/sI1ftJU2bNpX58+c7HYWalpYmb775prRo0SIvx4erJCcnyaYN38kTzw6WWnXrS2iZMHmkdz8JKV1WVn2xlPlyQ02aNpMBzw+Rlq3ud/VQkAu339VYOvd8Vuo1vu+6tzl7+qR89P5EefqF0VKgICcCdifvRs+Qf3TsLJWrVJVq1cNl1OtRcuL4cf3JFNyTKrKLlyiZsQUGFXP1kGzHsGlPd67++qviWh1IuW3bNklJSZGXXnpJdu3apZPuH3/8Me9HiQxpqamSlpYqPj4+TrPi41tI9vy2g5kC8hkVSMya+LpEdOkhZcpXcvVwcIsSE8/rfwMDA5lLN3X08CHp1r6l/u9ozdvrytPPDZHgkFBXDws2kKuku3bt2vL7779LkyZNpGPHjrrdpEuXLvoEOpUrV87RfanlBjdu3Ci7d+++5rqkpCSnRB0ifoX9pXqtOrLkw5lyJu6U/rhz3Zqv5Pfdv8rZM3FMEZDPrPr0QyngVUBadXjY1UNBHryBmvhWlNS9o55UrlKN+XRDNWrdLi+NGisTJk+XISNelePHjsrgZ3vJxQsXXD00WzEMw/QtP8r155zqXf4rr7xySw+uCvc2bdroVVDUBN17772yePFiCQ397zvOhIQEefLJJ+WJJ5647n0kJyfrLbOU5Cvi48G95aqXe+qbY6RPtwjx8ioglaqFy70tI+TP3/e4emgAMjm4f6+s/WKJvDp5br79jwCy782o1+XA/j/kg7kLmTY31fCephlfV65aXRfhj3aMkHVrV0u7B7u4dGzwfLk+Oc4PP/wgjz/+uNxzzz1y9OhRve/DDz/UqXV2jRgxQqfmJ0+elH379kmRIkV0eq6K8OyKiorSbwAybzOmvi2eLLRMORk/ZaZ8tPJHmbFkpbw1/UNJvXJFQkKdV5MB4Fp/7Noh5xPOyoinOsuzHe/V2+mTJ2TJ7H/LyD6deXncyFtRY2XjhvUSPXOeBAeHuHo4yCMBRYpK2bDyuuUE1hafXiZvbpt0b9myRerVq6fX5lY+/fRT6dmzp/To0UO2b9+ekTSrZPqNN96QlStXZuvB//Of/8i3334rJUuW1NuKFSvkueee0wdqqtPO+/v73/Q+IiMjZdiwYU77Dpy+InZQyM9Pb4nnz8nPWzdJr2cHu3pIADJp1KKt1LijgdOcTB41RO9v0ro9c+UG1JmW354wTtZ9961MnzlPypQh3PAkly5elGNHD8v9bTu4eii2Ytj0k79sF90jR46UL774QqfR48aNk/fee0+3fah2kHQqpVbX5aSfu2CmI/nVizB9+nQZOHCgNG/eXBYtWnTT+1BLFF69TKFPomf3Zv3803/EIQ4pU66CHD96WOa9N1nKhlWQlm0fdPXQkAsXL16Qw5k+3Tl69Ijs27tHigYGSmhoaeY0n0u6dFFOHj+ScTku9pgcOvC7+AcUlRK3hUhAUecD7tTqJYHFiktI2fIuGC1y6s03XpfVX38lb0+eKoX9/SUu7pTeHxBQRAoVKsSEupnpU96We5o2l+CQ0vq1nDdjmm7TbNmmrauHBhvIVtH9/PPPy+XLl3UhrJJt1QrSrFmza26nWjvi4+Oz/eDh4eF6BZQaNWo47Z86dar+98EHKSKzcvFConw4c6qcPhUrRYoESqNmLaVHnwFSsOB/P4mAe9m9a6f0fapXxuWJb03Q/3Z4sJOMGf/fr5G/+7bf/ueAjMtLZr2r/23csp08NfRVF44MeeHTpf8Nlvo9/b/fUWXUmDf0UoJwL3EnY2XcqyPkXEK8Xirw9rr1ZOqshRJUrLirh2YrXvYMusVwqM/Oskm1g6ge7kqVKskHH3wgrVu31sn3L7/8oveplUYmTJiQ5Uok1+vHVr3h12tHUa0mKlFXR4znxO5jnp1021H5kpyEwpPE/H3W1UNAHqpbLoj59CCJyfZo0bSTMkHOywy72pDle01/jMkdwyW/yVGvuSq4lWeeeUYGDx6s205US8ixY8dk4cKF8sILL0j//v2zfX+qH/tG/d/R0dE5LrgBAACQv5NuL5M3j1kyUPV3q2JYnSDn4sWLutVE9VW/+OKL8vTTT+f9KAEAAAA3lqtVVVS6/fLLL+szUO7cuVM2b94sp06d0j3dFStWzPtRAgAAwCMYNj05To6KbrU0oGoJueuuu/RKJao1pGbNmvoU8NWrV5cpU6bI0KFDzRstAAAA4IZy1F4yatQoef/99/UBlOqgym7duukzRqqk+5133tGXCxQoYN5oAQAA4Na88mcQnb+K7qVLl+oVStRSfqqtpE6dOnLlyhW9ekl+jfIBAAAAtyq6jxw5IvXr19dfq9O3q4MnVTsJBTcAAACyw7BpTpujnu7U1FTx8fnfWo/qbJIBAQFmjAsAAACwZ9KtzqPTu3fvjNOuJyUlSb9+/cTf39/pdp999lnejhIAAAAewcumUXeOiu5evZxPg/v444/n9XgAAAAAexfdc+bMMW8kAAAA8HheYk92fd4AAABA/j4NPAAAAJAbhj1bukm6AQAAALORdAMAAMAyXjaNuunpBgAAAExG0g0AAADLGPYMukm6AQAAALORdAMAAMAyXiTdAAAAAMxA0g0AAADLeNm0qZvVSwAAAACTkXQDAADAMoY9g26SbgAAAMBsJN0AAACwjBdJNwAAAAAzkHQDAADAMobYM+pm9RIAAADAZCTdAAAAsIyXPYNukm4AAADAbCTdAAAAsIwXSTcAAAAAM5B0AwAAwDKGTU9JyeolAAAAgMlIugEAAGAZL3sG3STdAAAAgNlIugEAAGAZg6QbAAAAgBlIugEAAGAZL5tG3axeAgAAAJiMpBsAAACW8bJn0E3SDQAAAJiNpBsAAACWMUi6AQAAAJiBpBsAAACW8RJ7Rt0U3QAsF+Tnw6x7kNiEJFcPAXmoTHE/5hMwAUU3AAAALGPYM+hm9RIAAADAbJwcBwAAAJau0+1l8pYTUVFR0qBBAylSpIjcdttt0qlTJ9m3b5/TbZKSkmTAgAFSokQJCQgIkK5du0psbGzOnnfOhgUAAAB4jvXr1+uCevPmzbJmzRq5fPmytGnTRi5cuJBxm6FDh8qKFStk6dKl+vbHjh2TLl265OhxDIfD4RAPs/vY/yYJnqF8ycKuHgLy0J+x/I56kkLe5DeehAMpPY+/T/5qov5g80HTH6Nvo/K5/t5Tp07pxFsV182aNZOEhAQpVaqULFq0SB566CF9m71790qNGjVk06ZN0qhRo2zdL38pAQAAgP+nimylePHi+t+YmBidfrdu3Tr9JhIeHi5hYWG66M4uVi8BAACAR61ekpycrLfMfH199XYjaWlpMmTIEGnSpInUrl1b7ztx4oT4+PhIUFCQ022Dg4P1ddlF0g0AAACPEhUVJYGBgU6b2nczqrd7586dsnjx4jwfE0k3AAAALONlQdQdGRkpw4YNc9p3s5R74MCB8uWXX8qGDRukbNmyGftDQkIkJSVF4uPjndJutXqJui67SLoBAADgUXx9faVo0aJO2/WKbrWmiCq4ly1bJt99951UrFjR6fr69euLt7e3rF27NmOfWlLw0KFD0rhx42yPiaQbAAAAtj0j5YABA/TKJMuXL9drdaf3aauWFD8/P/1vnz59dHKuDq5UBfygQYN0wZ3dlUsUim4AAADY1vTp0/W/9913n9P+OXPmSO/evfXXkyZNEi8vL31SHHWAZkREhERHR+focVinG26Bdbo9C+t0exbW6fYsrNPtefLbOt1ztx4y/TF6NwiT/IaebgAAAMBktJcAAADAMkZ+a+q2CEk3AAAAYDKSbgAAAFjGsOlcU3QDAADAo06Okx/RXgIAAACYjKQbAAAAljFsOtck3QAAAIDJSLoBAABgGcOmUTdJNwAAAGAykm4AAABYxrBp1E3SDQAAAJiMpBsAAACW8bLpXNv1eQMAAACWIekGAACAZQx6ugEAAACYgaQbAAAAljFsOtf0dAMAAAAmI+kGAACAZQx6ugEAAACYgaQbAAAAlvGy6Vzb9XkDAAAAliHpBgAAgGUMeroBAAAAmIGkGwAAAJYxbDrX9HQDAAAAJiPpBgAAgGUMm0bdJN0AAACAyUi6AQAAYBkvm3Z1k3QDAAAAJiPpdkOXLl6QRbOjZcvG7yXh7FmpWLW69Bn4olQNr+XqoSEXYrZtlflzZ8me3bsk7tQpeWfyVGnRqjVz6SZ2/7pdvlgyXw78sUfOno6TF8e8LXc3aZFxvcPhkI/nvSdrVy6TC4mJEl6rrjwzOFJCy4a5dNzIntTUVPlo7nvy/TcrJf7MaSlespS0eqCDdH/iGduuNezu+JvreoZNf3VIut3QtLdel1+2bZHBkWNl8uyP5Y67Gsno4f3l9KmTrh4aciHp0iWpVi1cRr48ivlzQ8lJl6R8pWrSZ9CILK9f/vE8+XrZYuk7+J8SNXWe+Bbyk3EjB0pKSrLlY0XOfbporqxc/on0GzJSoud/Jr2ffV4++2ierPj0I6bTTfE3F65C0u1mkpOTZNOG7yRy3ESpVbe+3vdI736y9T8bZNUXS6VHnwGuHiJyqEnTZnqDe7rz7iZ6y4pKub/6bJF07dFHGjS5T+8bOGKMPNOtjWz9cZ00aRFh8WiRU3t2/SKNmjSXBo2b6svBoaVl/dpV8sfeXUymm+JvrusZ9HTDHaSlpkpaWqr4+Pg47ffxLSR7ftvhsnEBuNbJ40d1S8Lt9Rpm7PMPKCJVatSWfbt/ZcrcQI1adeWX7T/J0cMH9eW/9u/Tf2vrN8z6jRYAXA9Jt5vxK+wv1WvVkSUfzpSy5StJYLHi8sN3q+T33b9KSJlyrh4egEziz57W/wYVK+40L0FBxXUxjvzvoR5PysWLidK/Z2fx8iqgQ4+eTw+Q++5v5+qhAW7LsGlPt8uL7j179sjmzZulcePGEh4eLnv37pUpU6ZIcnKyPP7449KyZcsbfr+6ndoyS0m+Ij6+vuKpVC/31DfHSJ9uEfo/ApWqhcu9LSPkz9/3uHpoAOBRNn7/jaxf87UMf/UNCatQWQ7s3yczp779/wdUPujq4QFwIy49kHLVqlVyxx13yPDhw+XOO+/Ul5s1ayb79++XgwcPSps2beS777674X1ERUVJYGCg0zZj6tviyULLlJPxU2bKRyt/lBlLVspb0z+U1CtXJCS0rKuHBiCToGIl9L/xZ884zUt8/BkJKv7f65C/zZk+WafdzVo9IBUqV5WWEf+Qjt16yNKFc1w9NMCt1+n2MnnLj1xadL/++uvy4osvyunTp2XOnDny2GOPyTPPPCNr1qyRtWvX6usmTJhww/uIjIyUhIQEp+2ZgcPFDgr5+UnxEqUk8fw5+XnrJrm7SXNXDwlAJreFltHF9c6ff8rYd/FCouzfs1Oq16zDXLnJwetXLw3o5eUljrQ0l40JgHtyaXvJrl27ZP78+frrhx9+WHr27CkPPfRQxvU9evTQxfiN+Pr66i0zn8QL4sl+/uk/4hCHlClXQY4fPSzz3pssZcMqSMu2fNTpji5evCCHDx3KuHz06BHZt3ePFA0MlNDQ0i4dG27u0qWLcuLo4YzLJ48f0wfbBRQpKqWCQ6V9l8fk04WzJKRMmNwWUlo+njtdipUolbGaCfK3Bvc0kyULZunXUreX/LFXPl+yQO5v18nVQ0Mu8TfX9Yz8GUSbznCoNa1cRLWCbN++XSpXrqwvFylSRH755RepVKmSvqxaTFSf96VLl3J0v7uPeXbR/eP338iHM6fK6VOxUqRIoDRq1lIvFahWRfBU5UsWFk+1besW6ftUr2v2d3iwk4wZf+NPetzVn7Ge8zu6a8c2GT382Wv2N2/zDxn40piMk+N8+9UyuZh4XsJr3yFPDx4ppcuWF09RyNvLowu0hbOiZdMP3+mTkalebtVq8kivvuLt7S2eqExxP/Fkdvyb6++Tv6rc1btPmf4YETVLSX7j0qK7bt268q9//UseeOABfXnnzp26yC5Y8L8B/A8//CC9evWSAwcO5Oh+Pb3otiNPLrrtyJOKbnh20W1Hnl5021F+K7q/2WN+0d2mRv4rul3aXtK/f399it10tWvXdrr+66+/vunqJQAAAEB+59Kk2ywk3Z6HpNuzkHR7FpJuz0LS7XnyW9K9Zk+c6Y9xf42Skt/wmSAAAADg6SfHAQAAgH145a/g3TIk3QAAAIDJSLoBAABgGSOfnjHSbCTdAAAAgMlIugEAAGAZw55BN0k3AAAAYDaSbgAAAFjGoKcbAAAAgBlIugEAAGAZL3q6AQAAAJiBpBsAAACWMejpBgAAAGAGkm4AAABYxqCnGwAAAIAZSLoBAABgGcOmc+3l6gEAAAAAno6kGwAAAJbxsmlTN0k3AAAAYDKSbgAAAFjGsOlck3QDAAAAJiPpBgAAgHUMe042STcAAABgMpJuAAAAWMawadRN0g0AAACYjKQbAAAAljHsGXSTdAMAAABmI+kGAACAZQybzjVFNwAAAKxj2HOyOZASAAAAMBlJNwAAACxj2DTqJukGAAAATEbSDQAAAMsY9gy6SboBAAAAs5F0AwAAwDKGTeeanm4AAADAZCTdAAAAsI5hz8km6QYAAABMRtINAAAAyxg2jbpJugEAAACTkXQDAADAMoY9g26SbgAAAMBsJN0AAACwjGHTuaanGwAAALa1YcMG6dChg5QuXVoMw5DPP//c6XqHwyGjRo2S0NBQ8fPzk9atW8sff/yR48fxyKS7TDE/Vw8BeayAl13fF3umysH+rh4CgOuIO5/C3HgY/xK+kq8Ykq9cuHBB6tatK0899ZR06dLlmuvffPNNeffdd2XevHlSsWJFefXVVyUiIkJ2794thQoVsnfRDQAAAGRH27Zt9ZYVlXJPnjxZXnnlFenYsaPeN3/+fAkODtaJ+COPPCLZRXsJAAAALF2n2zD5f3nlr7/+khMnTuiWknSBgYHSsGFD2bRpU47ui6QbAAAAHiU5OVlvmfn6+uotJ1TBrahkOzN1Of267CLpBgAAgKXrdBsmb1FRUTqRzrypfa5E0g0AAACPEhkZKcOGDXPal9OUWwkJCdH/xsbG6tVL0qnLd9xxR47ui6QbAAAAljEs2FSBXbRoUactN0W3Wq1EFd5r167N2Hfu3DnZsmWLNG7cOEf3RdINAAAA20pMTJT9+/c7HTy5Y8cOKV68uISFhcmQIUNk3LhxUrVq1YwlA9Wa3p06dcrR41B0AwAAwLbrdG/btk1atGiRcTm9LaVXr14yd+5ceemll/Ra3n379pX4+Hi59957ZdWqVTlao1sxHGoBQg+TcCnN1UNAHvP1phPKk6SmedyfHcBjcHIcz1M+n50cZ+fRRNMfo3aZAMlvSLoBAABgGSO/Rd0WIT4EAAAATEbSDQAAAMsY9gy6SboBAAAAs5F0AwAAwDKGTeeanm4AAADAZCTdAAAAsI5hz8km6QYAAABMRtINAAAAyxg2jbpJugEAAACTkXQDAADAMoY9g26SbgAAAMBsJN0AAACwjGHTuaanGwAAADAZSTcAAACsY9hzskm6AQAAAJORdAMAAMAyhk2jbpJuAAAAwGQk3QAAALCMYc+gm6QbAAAAMBtJNwAAACxj2HSu6ekGAAAATEbSDQAAAOsY9pxskm4AAADAZCTdAAAAsIxh06ibpBsAAAAwGUk3AAAALGPYM+gm6QYAAADMRtINAAAAyxg2nWt6ugEAAACTkXQDAADAOoY9J5ukGwAAADAZSTcAAAAsY9g06ibpBgAAAExG0e1m5s76QHo91k3uu6e+RLRoIsOHDJSDf//l6mHhFi1etFDa3t9SGtx5u/R4pJv89uuvzKmbitm2VQYP7CdtWjaVereHy/drv3X1kHALeD09T9ypWJkwOlK6PtBU/nFfA+n7eBf5fc8uVw/Ldut0GyZv+RFFt5vZHrNVunV/TGbNXyz/fm+WpF65LIP695FLly66emjIpVVfr5S334ySZ58bIIuXLpPq1cOl/7N95PTp08ypG0q6dEmqVQuXkS+PcvVQkAd4PT3L+XPnZOizvaRgwYIyfmK0zFi0TPoOGi4BRYq6emiwAXq63cy70TOcLo96PUoiWjaRPbt3Sb36DVw2LuTeh/PmSJeHHpZOnbvqy6+8NkY2bFgnn3/2qfR5pi9T62aaNG2mN3gGXk/PsmTBbCkVHCzDXxmbsS+0dFmXjsmODLGnfJd0OxwOVw/BrSQmntf/BgYGunooyIXLKSn6DVOjxvdk7PPy8pJGje6RX3/5mTkFgDy0aeM6qRpeS8a+/IJ0a9dc+vd6WFYu/4Q5hj2Lbl9fX9mzZ4+rh+EW0tLSZOJbUVL3jnpSuUo1Vw8HuXA2/qykpqZKiRIlnPary3FxccwpAOSh48eOyJfLlkiZcmESNek9+UfnhyV60r/km5XLmWcLGTbt6XZZe8mwYcOy3K8KkAkTJmQUIRMnTrzh/SQnJ+vNaV+aty7ePd2bUa/Lgf1/yAdzF7p6KAAA5HuOtDSpFl5Lnuo3WF+uUr2G/H1gv3y1bKm0adfR1cOzEUPsyGVF9+TJk6Vu3boSFBR0TXuJSrr9/f3FyMZblaioKBkzZozTvhH/HCWRr7wmnuytqLGyccN6eX/2hxIcHOLq4SCXigUVkwIFClxz0KS6XLJkSeYVAPJQ8RKlJKxiJad9YRUqysZ1rDIEDy6633jjDfnggw/knXfekZYtW2bs9/b2lrlz50rNmjWzdT+RkZHXpOZJad7iqdSbkrcnjJN1330r02fOkzJlOADEnXn7+EiNmrVky+ZN0rJV64y2oS1bNskjjz7u6uEBgEepVecOOXLob6d9Rw4flOCQUJeNyY4Mewbdriu6R44cKa1atZLHH39cOnTooBNrVXDnlGojubqVxHEpTTzVm2+8Lqu//krenjxVCvv7S1zcKb0/IKCIFCpUyNXDQy707PWkvPrPEVKrVm2pfXsdWfDhPLl06ZJ06tyF+XRDFy9ekMOHDmVcPnr0iOzbu0eKBgZKaGhpl44NOcfr6Vm6dO8pQ559Qj6aN0OatYqQfbt/0wdSDhnh2Z+OI38wHC5eLiQxMVEGDBggO3bskIULF0q9evX019lNurOS4MFF99131Mhy/6gxb8g/OnYWT+Xrne+O+c1THy1cIPPmzNJvoqqH15AR/3xF6tSpK54qNc1zVynatnWL9H2q1zX7OzzYScaMn+CSMSH37Ph6xp1PEU+2+cf1Mnv6FDl65JCEhJaRro/0lHYdHxJPVr5E/jrO7Vi8+T9jpYN8JL9xedGdbvHixTJkyBA5deqU/PbbbxTdsFXRbTeeXHQD7s7Ti247oujOH/LNyXEeeeQRuffeeyUmJkbKly/v6uEAAADABAY93a5XtmxZvQEAAACeJN8k3QAAAPB8hk3X6aZRFgAAADAZSTcAAACsY9hzskm6AQAAAJORdAMAAMAyhk3nmqQbAAAAMBlJNwAAACxj2DTqJukGAAAATEbSDQAAAMsYNu3qJukGAAAATEbSDQAAAOsY9pxskm4AAADAZCTdAAAAsIxh07km6QYAAABMRtINAAAAyxg2jbpJugEAAACTkXQDAADAMoZNu7pJugEAAACTkXQDAADAMoY9g26SbgAAAMBstJcAAAAAJqPoBgAAAExGTzcAAAAsY9DTDQAAAMAMJN0AAACwjME63QAAAADMQNINAAAAyxj0dAMAAAAwA0k3AAAALGPYdK5ZpxsAAAAwGUk3AAAArGPYc7JJugEAAACTkXQDAADAMoZNo26SbgAAAMBkJN0AAACwjGHPoJukGwAAADAbSTcAAAAsY9h0runpBgAAAExG0g0AAADrGPacbJJuAAAA2N60adOkQoUKUqhQIWnYsKH89NNPFN0AAABw33W6DZP/l1Mff/yxDBs2TF577TXZvn271K1bVyIiIuTkyZN597wdDodDPEzCpTRXDwF5zNebD2U8SWqax/3ZATxG3PkUVw8Beax8Cd98NaeXLpv/GH7eObu9SrYbNGggU6dO1ZfT0tKkXLlyMmjQIBk5cmSejIlKBgAAAJau022YvOVESkqKxMTESOvWrTP2eXl56cubNm3Ks+fNgZQAAADwKMnJyXrLzNfXV29Xi4uLk9TUVAkODnbary7v3bs3z8bkkUV3oJ/nB/jqBykqKkoiIyOz/AGC+7HXa+r5h67b6/X0fHZ6Pf3zWSuCWez0muY3hSyoPkePi5IxY8Y47VP92qNHjxZX8ciebjs4d+6cBAYGSkJCghQtWtTVw0Ee4DX1LLyenoXX0/Pwmnq25Bwk3aq9pHDhwvLJJ59Ip06dMvb36tVL4uPjZfny5XkyJs+PhAEAAGArvr6+OpTMvF3vEw0fHx+pX7++rF27NmOfOpBSXW7cuHGejckj20sAAACA7FLLBapk+6677pK7775bJk+eLBcuXJAnn3xS8gpFNwAAAGyte/fucurUKRk1apScOHFC7rjjDlm1atU1B1feCopuN6U+IlEHBHDwh+fgNfUsvJ6ehdfT8/Ca4moDBw7Um1k4kBIAAAAwGQdSAgAAACaj6AYAAABMRtENAAAAmIyi201NmzZNKlSoIIUKFZKGDRvKTz/95OohIZc2bNggHTp0kNKlS4thGPL5558zl25MneGuQYMGUqRIEbntttv0iRb27dvn6mEhl6ZPny516tTJWOdXrdn79ddfM58eYsKECfrv7pAhQ1w9FNgARbcb+vjjj/V6kmr1ku3bt0vdunUlIiJCTp486eqhIRfUOqDqNVRvpOD+1q9fLwMGDJDNmzfLmjVr5PLly9KmTRv9OsP9lC1bVhdmMTExsm3bNmnZsqV07NhRdu3a5eqh4RZt3bpV3n//ff2mCrACq5e4IZVsqyRt6tSpGWdNKleunAwaNEhGjhzp6uHhFqjEZdmyZU6noYV7U+u+qsRbFePNmjVz9XCQB4oXLy5vvfWW9OnTh/l0U4mJiVKvXj2Jjo6WcePG6TWZ1clQADORdLuZlJQUnbi0bt06Y5+Xl5e+vGnTJpeODcC1EhISMgo1uLfU1FRZvHix/tQiL08NDeupT6Pat2/v9N9SwGycHMfNxMXF6T/8V58hSV3eu3evy8YF4FrqUyjVK9qkSROpXbs2U+SmfvvtN11kJyUlSUBAgP40qmbNmq4eFnJJvXFSrZmqvQSwEkU3AJiYpu3cuVM2btzIHLux6tWry44dO/SnFp988on06tVLtwtReLufw4cPy+DBg/XxFmohAsBKFN1upmTJklKgQAGJjY112q8uh4SEuGxcAJypUwl/+eWXenUadTAe3JePj49UqVJFf12/fn2dkE6ZMkUfhAf3otoz1aIDqp87nfr0WP2equOkkpOT9X9jATPQ0+2Gf/zVH/21a9c6fYStLtNjCLiew+HQBbdqQfjuu++kYsWKrh4S8pj6m6uKM7ifVq1a6XYh9clF+nbXXXdJjx499NcU3DATSbcbUssFqo831R+Ku+++Wx9xrQ7sefLJJ109NOTyKPr9+/dnXP7rr7/0H3914F1YWBhz6oYtJYsWLZLly5frtbpPnDih9wcGBoqfn5+rh4ccioyMlLZt2+rfxfPnz+vXdt26dbJ69Wrm0g2p38mrj6/w9/eXEiVKcNwFTEfR7Ya6d++ulyEbNWqU/g+6Wupo1apV1xxcCfeg1v5t0aKF05sqRb2xmjt3rgtHhtyeTEW57777nPbPmTNHevfuzaS6GdWK8MQTT8jx48f1Gye1prMquO+//35XDw2Am2GdbgAAAMBk9HQDAAAAJqPoBgAAAExG0Q0AAACYjKIbAAAAMBlFNwAAAGAyim4AAADAZBTdAAAAgMkougEgD/39998ybtw4faZRAADSUXQDQB5JTk6Wbt26ScmSJSUgIOCGt1Vnp+zUqVPGZXUGyyFDhvBaAICHougGgKuKYcMw9Obj4yNVqlSR119/Xa5cuXLTeRo6dKi0adNG+vXrl+M5/eyzz2Ts2LEZlytUqCCTJ0/mtQEAD1HQ1QMAgPzmgQcekDlz5ujkeuXKlTJgwADx9vaWyMhIp9ulpKTowjxddHR0rh+zePHitzRmAED+RtINAFfx9fWVkJAQKV++vPTv319at24tX3zxRUZLyPjx46V06dJSvXp1ffvDhw/Lww8/LEFBQbp47tixo+7tTpeamirDhg3T15coUUJeeuklcTgcTo+Zub1EfX3w4EGdnKen7gAA90bRDQA34efnp1NtZe3atbJv3z5Zs2aNfPnll3L58mWJiIiQIkWKyA8//CA//vij7udWaXn697zzzjsyd+5cmT17tmzcuFHOnDkjy5Ytu2GrSdmyZXVby/Hjx/UGAHBvtJcAwHWoNFoV2atXr5ZBgwbJqVOnxN/fX2bOnJnRVrJgwQJJS0vT+9ITadWaolLtdevW6R5v1ZutWlO6dOmir3/vvff0fV6PSssLFCigC3mVuAMA3B9FNwBcRSXYKq1WKbYqqB977DEZPXq07u2+/fbbnfq4f/nlF9m/f78ukDNLSkqSP//8UxISEnRS3bBhw//94S1YUO66665rWkwAAJ6LohsArtKiRQuZPn26Lq5V77YqktOppDsztR53/fr1ZeHChdfMY6lSpZhbAIBG0Q0AV1GFtVoqMDvq1asnH3/8sdx2221StGjRLG8TGhoqW7ZskWbNmunLavnBmJgY/b3Xowp+dQAmAMAzcCAlANyCHj166JPhqBVL1IGUf/31l+7lfv755+XIkSP6NoMHD5YJEybI559/Lnv37pXnnntO4uPjb3i/ap3uDRs2yNGjRyUuLo7XCADcHEU3ANyCwoUL6+I4LCxMHyhZo0YN6dOnj+7pTk++X3jhBenZs6f06tVLGjdurPu/O3fufMP7VSuXqGUHK1euTJsKAHgAw8GRPAAAAICpSLoBAAAAk1F0AwAAACaj6AYAAABMRtENAAAAmIyiGwAAADAZRTcAAABgMopuAAAAwGQU3QAAAIDJKLoBAAAAk1F0AwAAACaj6AYAAABMRtENAAAAiLn+D4ess5fRM61vAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "final_metrics = evaluate(model, val_loader, criterion, DEVICE)\n",
        "y_true, y_pred = final_metrics[\"y_true\"], final_metrics[\"y_pred\"]\n",
        "\n",
        "print(f\"Loss: {final_metrics['loss']:.4f}\")\n",
        "print(f\"Accuracy: {final_metrics['acc']:.4f}\")\n",
        "print(f\"Macro Recall: {final_metrics['recall_macro']:.4f}\")\n",
        "print(f\"Macro F1: {final_metrics['f1_macro']:.4f}\")\n",
        "\n",
        "\n",
        "cm = final_metrics[\"confusion_matrix\"]\n",
        "\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Prédit')\n",
        "plt.ylabel('Réel')\n",
        "plt.title('Matrice de confusion – CNN')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "DOK7wALXES7v",
        "outputId": "803b1932-fde6-491c-ddb5-f3fefc371ccf"
      },
      "outputs": [],
      "source": [
        "# Chargement des données de test depuis le fichier pickle\n",
        "with open(TEST_PATH, \"rb\") as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "# Extraction des images de test \n",
        "test_images = test_data[\"images\"].astype(np.uint8)\n",
        "\n",
        "# Création du dataset de test\n",
        "test_dataset = RetinaDataset(\n",
        "    test_images,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "# DataLoader pour l'inférence sur l'ensemble de test\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Initialisation du modèle CNN et chargement des poids entraînés\n",
        "model = CNN(num_classes=5)\n",
        "model.load_state_dict(\n",
        "    torch.load(MODEL_PATH, map_location=DEVICE)\n",
        ")\n",
        "\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# Liste pour stocker toutes les prédictions\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs in test_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "\n",
        "        # Propagation avant\n",
        "        outputs = model(imgs)\n",
        "\n",
        "        # Prédiction des classes\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        # Stockage des prédictions\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Conversion des prédictions en tableau NumPy\n",
        "all_preds = np.array(all_preds)\n",
        "\n",
        "# Création du DataFrame pour la soumission Kaggle\n",
        "df = pd.DataFrame({\n",
        "    \"ID\": np.arange(1, len(all_preds) + 1),\n",
        "    \"Label\": all_preds\n",
        "})\n",
        "\n",
        "# Sauvegarde du fichier CSV de soumission\n",
        "df.to_csv(\"IFT3395_YAPS_MCS_V61.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmaeCyIOMvNE"
      },
      "source": [
        "Afin de gérer la variance entre entraînement, on peut runner l'entraînement plusieurs fois avec des seeds différents (exemple, 0, 1, 17, 21, 42). La section ci-dessous pourra être décommenter si l'on souhait utiliser cette stratégie. Il sera nécessairement au préalable de run l'entraînement le nombre de fois voulu et en eregistrant un checkpoint pour chaque run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "VLs6XW2IMIeq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nMODEL_PATHS = [\\n    \"best_model_1.pth\",\"best_model_2.pth\",\"best_model_3.pth\",\"best_model_4.pth\",\"best_model_5.pth\",\\n\\n]\\n'"
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# On récupère les checkpoints pour chaque run différents.\n",
        "\"\"\"\n",
        "MODEL_PATHS = [\n",
        "    \"best_model_1.pth\",\"best_model_2.pth\",\"best_model_3.pth\",\"best_model_4.pth\",\"best_model_5.pth\",\n",
        "\n",
        "]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "J9dapHwHMSbn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nwith open(TEST_PATH, \"rb\") as f:\\n    test_data = pickle.load(f)\\nX_test = test_data[\"images\"] if isinstance(test_data, dict) else test_data\\ntest_loader = DataLoader(RetinaDataset(X_test, transform=val_transform), batch_size=BATCH_SIZE, shuffle=False)\\n'"
            ]
          },
          "execution_count": 285,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "with open(TEST_PATH, \"rb\") as f:\n",
        "    test_data = pickle.load(f)\n",
        "X_test = test_data[\"images\"] if isinstance(test_data, dict) else test_data\n",
        "test_loader = DataLoader(RetinaDataset(X_test, transform=val_transform), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RULQDv4yMXWB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nmodels = []\\nfor path in MODEL_PATHS:\\n    try:\\n        m = CNN(num_classes=5).to(DEVICE)\\n        m.load_state_dict(torch.load(path, map_location=DEVICE))\\n        m.eval()\\n        models.append(m)\\n        print(f\"✓ Chargé : {path}\")\\n    except Exception as e:\\n        print(f\"Erreur sur {path}: {e}\")\\n\\n'"
            ]
          },
          "execution_count": 286,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "models = []\n",
        "for path in MODEL_PATHS:\n",
        "    try:\n",
        "        m = CNN(num_classes=5).to(DEVICE)\n",
        "        m.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "        m.eval()\n",
        "        models.append(m)\n",
        "        print(f\"Chargé : {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur sur {path}: {e}\")\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMJVnsWeMauo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nfinal_preds = []\\n\\nprint(\"Démarrage de l\\'ensemble...\")\\nwith torch.no_grad():\\n    for imgs in test_loader:\\n        imgs = imgs.to(DEVICE)\\n\\n        # On va stocker la somme des probabilités pour ce batch\\n        # Forme : [Batch_Size, 5 Classes]\\n        batch_probs_sum = torch.zeros(imgs.size(0), 5).to(DEVICE)\\n\\n        for model in models:\\n            logits = model(imgs)\\n            # IMPORTANT : On transforme les logits en probabilités (0.0 à 1.0)\\n            probs = F.softmax(logits, dim=1)\\n            batch_probs_sum += probs\\n\\n        # On fait la moyenne (diviser par 5)\\n        avg_probs = batch_probs_sum / len(models)\\n\\n        # On prend la classe qui a la plus haute probabilité moyenne\\n        preds = torch.argmax(avg_probs, dim=1)\\n        final_preds.extend(preds.cpu().numpy())\\n'"
            ]
          },
          "execution_count": 287,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "final_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs in test_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "\n",
        "        batch_probs_sum = torch.zeros(imgs.size(0), 5).to(DEVICE)\n",
        "\n",
        "        for model in models:\n",
        "            logits = model(imgs)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            batch_probs_sum += probs\n",
        "\n",
        "        avg_probs = batch_probs_sum / len(models)\n",
        "\n",
        "        preds = torch.argmax(avg_probs, dim=1)\n",
        "        final_preds.extend(preds.cpu().numpy())\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0OZjKj7McfX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\ndf = pd.DataFrame({\"ID\": np.arange(1, len(final_preds) + 1), \"Label\": final_preds})\\ndf.to_csv(\"IFT3365_YAPS_MCS_V62.csv\", index=False)\\nprint(\"Fichier \\'submission_ensemble_5seeds.csv\\' généré avec succès.\")\\n\\n'"
            ]
          },
          "execution_count": 288,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "df = pd.DataFrame({\"ID\": np.arange(1, len(final_preds) + 1), \"Label\": final_preds})\n",
        "df.to_csv(\"IFT3365_YAPS_MCS_V62.csv\", index=False)\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqGMFa2CMSHT"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "kaggle2 (3.13.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

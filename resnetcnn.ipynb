{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "217fd454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import os\n",
    "from pyclass import TransformSubset, TinyCNN\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, recall_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "927708c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 \n",
    "\n",
    "random.seed(SEED)     \n",
    "np.random.seed(SEED)     \n",
    "torch.manual_seed(SEED)  \n",
    "torch.cuda.manual_seed(SEED)     \n",
    "torch.cuda.manual_seed_all(SEED)   \n",
    "torch.backends.cudnn.benchmark = False \n",
    "torch.backends.cudnn.deterministic = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "e1871146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLKDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        self.images = data['images']\n",
    "        self.labels = data['labels'].reshape(-1)\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "  \n",
    "        \n",
    "        image = Image.fromarray(image.astype('uint8')) \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "9aceaad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    use_mps = True\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_mps = False\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "33f2b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PLKDataset('ift-3395-6390-kaggle-2-competition-fall-2025/train_data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "cb903890",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images, raw_labels = dataset.images, dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "90a3210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = dataset.labels\n",
    "idx = np.arange(len(dataset))\n",
    "train_idx, valid_idx = train_test_split(idx, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "017c2b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR_MEAN= [0.2111530601978302, 0.005323430523276329, 0.22947929799556732]\n",
      "IR_STD= [0.18979966640472412, 0.016638699918985367, 0.16980312764644623]\n"
     ]
    }
   ],
   "source": [
    "X_train_raw = dataset.images[train_idx]\n",
    "\n",
    "X_train_float =X_train_raw.astype(np.float32) / 255.0\n",
    "\n",
    "means = X_train_float.mean(axis=(0,1,2))\n",
    "std = X_train_float.std(axis=(0,1,2))\n",
    "\n",
    "IR_MEAN = means.tolist()\n",
    "IR_STD = std.tolist()\n",
    "\n",
    "print(\"IR_MEAN=\", IR_MEAN)\n",
    "print(\"IR_STD=\", IR_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "4040808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# 1. Custom function to fix the channels\n",
    "def enhance_channels(img_tensor):\n",
    "    r = img_tensor[0]\n",
    "    b = img_tensor[2]\n",
    "    \n",
    "\n",
    "    diff = (r - b) + 0.5\n",
    "    diff = torch.clamp(diff, 0, 1)\n",
    "\n",
    "    return torch.stack([r, diff, b], dim=0)\n",
    "\n",
    "# 2. Training Transforms (Aggressive)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),      \n",
    "    transforms.RandomRotation(180),        \n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.9, 1.15)), \n",
    "    \n",
    "\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.3),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Lambda(enhance_channels), \n",
    "    \n",
    "    transforms.Normalize(mean=IR_MEAN, std=IR_STD)\n",
    "])\n",
    "\n",
    "# 3. Validation Transforms (Deterministic)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Lambda(enhance_channels),\n",
    "    transforms.Normalize(mean=IR_MEAN, std=IR_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "bf7dd465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "train_data = TransformSubset(train_dataset, train_transform)\n",
    "\n",
    "val_dataset = Subset(dataset, valid_idx)\n",
    "val_data = TransformSubset(val_dataset, val_transform)\n",
    "\n",
    "\n",
    "y_train_idx = train_dataset.indices\n",
    "all_labels = dataset.labels[y_train_idx]\n",
    "class_count = np.bincount(all_labels.astype(int))\n",
    "weights_class =   1. / (class_count)\n",
    "sample_weights = [weights_class[int(label)] for label in all_labels]\n",
    "sample_weights = torch.from_numpy(np.array(sample_weights)).double()\n",
    "\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=64, shuffle=False, pin_memory=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "315830c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "39eaa3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "train_labels = dataset.labels[train_idx]\n",
    "classes = np.unique(train_labels)\n",
    "class_weights = compute_class_weight('balanced', classes=classes,y=train_labels)\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "weights_tensor[1] *= 1.1\n",
    "weights_tensor[2] *= 0.9\n",
    "\n",
    "weights_tensor[3] *= 1.15\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "08c188a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor, label_smoothing=0.1)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                    lr=2e-4, \n",
    "                    weight_decay=10e-2\n",
    ")\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max = 50, eta_min =1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "3e8f6645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss = 1.7285\n",
      "Val Loss = 1.6812, Bal Acc = 0.2201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss = 1.7103\n",
      "Val Loss = 1.6637, Bal Acc = 0.2537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss = 1.6510\n",
      "Val Loss = 1.6028, Bal Acc = 0.3045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss = 1.6370\n",
      "Val Loss = 1.5908, Bal Acc = 0.3547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss = 1.6102\n",
      "Val Loss = 1.5849, Bal Acc = 0.3262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss = 1.6222\n",
      "Val Loss = 1.5890, Bal Acc = 0.3254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss = 1.6013\n",
      "Val Loss = 1.5913, Bal Acc = 0.3477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss = 1.6127\n",
      "Val Loss = 1.5877, Bal Acc = 0.3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss = 1.6015\n",
      "Val Loss = 1.5886, Bal Acc = 0.3596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss = 1.5993\n",
      "Val Loss = 1.5776, Bal Acc = 0.3817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss = 1.5722\n",
      "Val Loss = 1.5800, Bal Acc = 0.3370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss = 1.5741\n",
      "Val Loss = 1.5792, Bal Acc = 0.3269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss = 1.5574\n",
      "Val Loss = 1.5857, Bal Acc = 0.3587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train Loss = 1.5641\n",
      "Val Loss = 1.5821, Bal Acc = 0.3746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss = 1.5526\n",
      "Val Loss = 1.5967, Bal Acc = 0.3139\n",
      "Arrêt précoce\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    running_train_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    preds, gts = [], []\n",
    "    running_val_loss = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "          \n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss += val_loss.item() \n",
    "\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            gts.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader) \n",
    "    \n",
    "    # Calcul des métriques\n",
    "    all_preds = np.array(preds)\n",
    "    all_labels = np.array(gts)\n",
    "\n",
    "    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Val Loss = {avg_val_loss:.4f}, Bal Acc = {bal_acc:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "         best_val_loss = avg_val_loss\n",
    "         torch.save(model.state_dict(), 'best_model_cnn_ir.pth')\n",
    "         patience_counter = 0\n",
    "    else:\n",
    "         patience_counter += 1\n",
    "         if patience_counter >= 5:\n",
    "             print(\"Arrêt précoce\")\n",
    "             break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "b656daeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Balanced Accuracy: 0.3139\n",
      "Validation Recall: 0.3139\n",
      "Validation Accuracy: 0.3333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7091    0.4588    0.5571        85\n",
      "           1     0.2250    0.3750    0.2812        24\n",
      "           2     0.1333    0.0541    0.0769        37\n",
      "           3     0.2812    0.2647    0.2727        34\n",
      "           4     0.1000    0.4167    0.1613        12\n",
      "\n",
      "    accuracy                         0.3333       192\n",
      "   macro avg     0.2897    0.3139    0.2699       192\n",
      "weighted avg     0.4238    0.3333    0.3550       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds, average='macro')\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Validation Balanced Accuracy: {bal_acc:.4f}\")\n",
    "print(f\"Validation Recall: {recall:.4f}\")\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "2cc9f1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "0a199ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"num_classes\": 5,\n",
    "}, \"cnnnet.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "05c8887a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1152, out_features=128, bias=True)\n",
       "  (bn_fc): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load(\"cnnnet.pth\", map_location=device, weights_only=False)\n",
    "\n",
    "model = TinyCNN()\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "0039d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pickle.load(open('ift-3395-6390-kaggle-2-competition-fall-2025/test_data.pkl', 'rb'))\n",
    "test_images = test_dataset['images']\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    \n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    transforms.Normalize(mean=IR_MEAN, std=IR_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "98596f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TestPKLDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "     \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "\n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "84db4991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamira.poldosilva/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (64x8192 and 1152x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[358]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[32m      7\u001b[39m     images = images.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     _, predicted = torch.max(outputs, \u001b[32m1\u001b[39m)\n\u001b[32m     10\u001b[39m     preds.extend(predicted.cpu().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDEM/A25/IFT3395/kaggle2/pyclass.py:121\u001b[39m, in \u001b[36mTinyCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    118\u001b[39m x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Classifier\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.bn_fc(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    122\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m    123\u001b[39m x = \u001b[38;5;28mself\u001b[39m.fc2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDEM/A25/IFT3395/kaggle2/kaggle2/lib/python3.13/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: linear(): input and weight.T shapes cannot be multiplied (64x8192 and 1152x128)"
     ]
    }
   ],
   "source": [
    "test_ds = TestPKLDataset(test_images, transform=test_transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, pin_memory=True)\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "df = pd.DataFrame({\n",
    "\n",
    "    \"ID\": np.arange(1, len(preds) + 1),\n",
    "    \"Label\": preds\n",
    "})\n",
    "\n",
    "df.to_csv(\"IFT3395_YAPS_MCSV53.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle2 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

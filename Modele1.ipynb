{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c71bed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651f43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "434ea9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_features(img):\n",
    "    \"\"\"\n",
    "    img : (28,28,3)\n",
    "    Retourne un vecteur de features ~ 3+3+18 dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Convert to float\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    # Gray\n",
    "    gray = img.mean(axis=2)\n",
    "\n",
    "    # ----- RGB stats -----\n",
    "    for c in range(3):\n",
    "        features.append(img[:, :, c].mean())\n",
    "        features.append(img[:, :, c].std())\n",
    "\n",
    "    # ----- LBP -----\n",
    "    def lbp(gray):\n",
    "        H, W = gray.shape\n",
    "        padded = np.pad(gray, ((1, 1), (1, 1)), mode=\"edge\")\n",
    "\n",
    "        lbp = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "        offsets = [\n",
    "            (-1, -1), (-1, 0), (-1, 1),\n",
    "            ( 0, -1),          (0, 1),\n",
    "            ( 1, -1), ( 1, 0), (1, 1)\n",
    "        ]\n",
    "\n",
    "        for idx, (dy, dx) in enumerate(offsets):\n",
    "            neigh = padded[1+dy:H+1+dy, 1+dx:W+1+dx]\n",
    "\n",
    "            # cast pour éviter ton erreur\n",
    "            bit = (neigh >= gray).astype(np.uint8)  \n",
    "\n",
    "            lbp |= (bit << idx)\n",
    "\n",
    "        return lbp\n",
    "\n",
    "\n",
    "    lbp_img = lbp(gray)\n",
    "    hist_lbp = np.histogram(lbp_img, bins=16, range=(0, 256))[0]\n",
    "    features.extend(hist_lbp.tolist())\n",
    "\n",
    "    return np.array(features, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "977b0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    def fit(self, X):\n",
    "        self.mu = X.mean(axis=0)\n",
    "        self.sigma = X.std(axis=0) + 1e-8\n",
    "    def transform(self, X):\n",
    "        return (X - self.mu) / self.sigma\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e6ea9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier:\n",
    "    def __init__(self, input_dim, num_classes, reg=0.0):\n",
    "        self.W = 0.01 * np.random.randn(input_dim, num_classes).astype(np.float32)\n",
    "        self.reg = reg  # L2\n",
    "        self.b = np.zeros(num_classes)\n",
    "\n",
    "    def _softmax(self, scores):\n",
    "        # scores: (N, K)\n",
    "        scores = scores - scores.max(axis=1, keepdims=True)  # stabilité num.\n",
    "        exp_scores = np.exp(scores)\n",
    "        return exp_scores / exp_scores.sum(axis=1, keepdims=True)\n",
    "    def loss_and_grad(self, X, y, sample_weights=None):\n",
    "        N = X.shape[0]\n",
    "\n",
    "        scores = X @ self.W + self.b\n",
    "        probs = self._softmax(scores)\n",
    "\n",
    "        correct_logprobs = -np.log(probs[np.arange(N), y] + 1e-12)\n",
    "\n",
    "        if sample_weights is None:\n",
    "            sample_weights = np.ones(N)\n",
    "\n",
    "        loss = np.sum(sample_weights * correct_logprobs) / N\n",
    "        loss += 0.5 * self.reg * np.sum(self.W * self.W)\n",
    "\n",
    "        dscores = probs\n",
    "        dscores[np.arange(N), y] -= 1\n",
    "\n",
    "        dscores *= sample_weights[:, None]\n",
    "        dscores /= N\n",
    "\n",
    "        dW = X.T @ dscores + self.reg * self.W\n",
    "        db = dscores.sum(axis=0)\n",
    "\n",
    "        return loss, dW, db\n",
    "\n",
    "\n",
    "    def fit(self, X, y, lr=1e-4, n_steps=1000, sample_weights=None, verbose=True):\n",
    "        losses = []\n",
    "        for step in range(n_steps):\n",
    "            loss, dW, db = self.loss_and_grad(X, y, sample_weights)\n",
    "            self.W -= lr * dW\n",
    "            self.b -= lr * db\n",
    "            losses.append(loss)\n",
    "\n",
    "            if verbose and step % 100 == 0:\n",
    "                print(f\"Step {step}, loss = {loss:.4f}\")\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        scores = X @ self.W\n",
    "        probs = self._softmax(scores)\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return probs.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b62ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return np.mean(y_true == y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e47462c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_np(y_true, y_pred, num_classes=None):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = max(y_true.max(), y_pred.max()) + 1\n",
    "\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a08a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(y_true, y_pred):\n",
    "    cm = confusion_matrix_np(y_true, y_pred)\n",
    "    TP = np.diag(cm)\n",
    "    real_pos = cm.sum(axis=1)\n",
    "    recall = np.where(real_pos > 0, TP / real_pos, 0.0)\n",
    "    return recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "986a70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_per_class(cm):\n",
    "    \"\"\"\n",
    "    cm : matrice de confusion (numpy array KxK)\n",
    "    retourne un vecteur de recall par classe\n",
    "    \"\"\"\n",
    "    TP = np.diag(cm)\n",
    "    real_pos = cm.sum(axis=1)   # total de vrais échantillons par classe\n",
    "    \n",
    "    # recall par classe (évite division par zéro)\n",
    "    recall = np.where(real_pos > 0, TP / real_pos, 0.0)\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12477291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss = 0.2204\n",
      "Step 100, loss = 0.2202\n",
      "Step 200, loss = 0.2200\n",
      "Step 300, loss = 0.2197\n",
      "Step 400, loss = 0.2195\n",
      "Step 500, loss = 0.2193\n",
      "Step 600, loss = 0.2190\n",
      "Step 700, loss = 0.2188\n",
      "Step 800, loss = 0.2186\n",
      "Step 900, loss = 0.2184\n",
      "Step 1000, loss = 0.2182\n",
      "Step 1100, loss = 0.2180\n",
      "Step 1200, loss = 0.2178\n",
      "Step 1300, loss = 0.2176\n",
      "Step 1400, loss = 0.2174\n",
      "Step 1500, loss = 0.2172\n",
      "Step 1600, loss = 0.2170\n",
      "Step 1700, loss = 0.2168\n",
      "Step 1800, loss = 0.2167\n",
      "Step 1900, loss = 0.2165\n",
      "Step 2000, loss = 0.2163\n",
      "Step 2100, loss = 0.2161\n",
      "Step 2200, loss = 0.2160\n",
      "Step 2300, loss = 0.2158\n",
      "Step 2400, loss = 0.2156\n",
      "Step 2500, loss = 0.2155\n",
      "Step 2600, loss = 0.2153\n",
      "Step 2700, loss = 0.2152\n",
      "Step 2800, loss = 0.2150\n",
      "Step 2900, loss = 0.2149\n",
      "Step 3000, loss = 0.2147\n",
      "Step 3100, loss = 0.2146\n",
      "Step 3200, loss = 0.2144\n",
      "Step 3300, loss = 0.2143\n",
      "Step 3400, loss = 0.2142\n",
      "Step 3500, loss = 0.2140\n",
      "Step 3600, loss = 0.2139\n",
      "Step 3700, loss = 0.2138\n",
      "Step 3800, loss = 0.2136\n",
      "Step 3900, loss = 0.2135\n",
      "Step 4000, loss = 0.2134\n",
      "Step 4100, loss = 0.2132\n",
      "Step 4200, loss = 0.2131\n",
      "Step 4300, loss = 0.2130\n",
      "Step 4400, loss = 0.2129\n",
      "Step 4500, loss = 0.2128\n",
      "Step 4600, loss = 0.2126\n",
      "Step 4700, loss = 0.2125\n",
      "Step 4800, loss = 0.2124\n",
      "Step 4900, loss = 0.2123\n",
      "Test accuracy = 0.2962962962962963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nrec = recall_per_class(cm)\\nprint(\"Accuracy        :\", acc)\\nprint(\"Balanced acc    :\", bal_acc)\\nprint(\"Confusion matrix:\\n\", cm)\\nprint(\"Recall par classe :\", rec)\\nprint(\"Recall moyen (macro):\", rec.mean())'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "path_to_data = 'ift-3395-6390-kaggle-2-competition-fall-2025/train_data.pkl'\n",
    "\n",
    "# --- Load training data ---\n",
    "with open(path_to_data, \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "X_imgs = train_data[\"images\"]\n",
    "y = train_data[\"labels\"].reshape(-1)\n",
    "\n",
    "# --- Feature extraction ---\n",
    "X = np.array([extract_features(img) for img in X_imgs], dtype=np.float32)\n",
    "\n",
    "# --- Normalize ---\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# --- Split ---\n",
    "n_train = int(0.8 * len(X))\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "# --- Class weights ---\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights /= class_weights.sum()\n",
    "sample_weights = class_weights[y_train]\n",
    "\n",
    "# --- Train model ---\n",
    "num_classes = len(np.unique(y))\n",
    "model = SoftmaxClassifier(input_dim=X.shape[1], num_classes=num_classes, reg=1e-3)\n",
    "\n",
    "model.fit(X_train, y_train, lr=1e-3, n_steps=5000, sample_weights=sample_weights)\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = model.predict(X_test)\n",
    "acc = (y_pred == y_test).mean()\n",
    "print(\"Test accuracy =\", acc)\n",
    "\n",
    "# --- Save model ---\n",
    "pickle.dump((model, scaler), open(\"model_softmax.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "#print(\"Train accuracy:\", acc)\n",
    "print(\"Test accuracy:\", acc)\n",
    "#cm = confusion_matrix_np(y_true, y_pred)\n",
    "#acc = accuracy(y_true, y_pred)\n",
    "#bal_acc = balanced_accuracy(y_true, y_pred)\n",
    "\n",
    "\"\"\"\n",
    "rec = recall_per_class(cm)\n",
    "print(\"Accuracy        :\", acc)\n",
    "print(\"Balanced acc    :\", bal_acc)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(\"Recall par classe :\", rec)\n",
    "print(\"Recall moyen (macro):\", rec.mean())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934faf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ift-3395-6390-kaggle-2-competition-fall-2025/test_data.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "X_test_imgs = test_data[\"images\"]\n",
    "\n",
    "X_test_feats = np.array([extract_features(img) for img in X_test_imgs], dtype=np.float32)\n",
    "\n",
    "# normaliser avec les stats du train\n",
    "X_test_norm = (X_test_feats - mean) / std\n",
    "\n",
    "# prédictions\n",
    "y_pred = model.predict(X_test_norm).astype(int)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle2 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
